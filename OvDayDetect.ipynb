{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please input organisation name from this list: \n",
      "0                         HRA Ltd\n",
      "1                             PAL\n",
      "2                     Juventus FC\n",
      "3             Hormonix Onboarding\n",
      "4                     The Well HQ\n",
      "5                 Manchester City\n",
      "6                   Mint Internal\n",
      "7             Hartpury University\n",
      "8          Test Organisation Name\n",
      "9                         Boxford\n",
      "10                     Venezia FC\n",
      "11    English Institute of Sports\n",
      "12           Testing Organisation\n",
      "13                         AEK FC\n",
      "14                            RDR\n",
      "15                    Optima-life\n",
      "16             Army Health Branch\n",
      "17                     LboroSSEHS\n",
      "18            RGBH Uni Portsmouth\n",
      "19                         Jennis\n",
      "20                       Test Org\n",
      "21                   FC Barcelona\n",
      "Name: name, dtype: object\n",
      "Organisation Name: Venezia FC\n",
      "Organisation code: VNZ\n",
      "...\n",
      "Do you want to choose a programme from this organisation? (y/n)\n",
      "This organisation has 6 participants\n",
      "Please input participant name from this list | or type ALL to generate batch report: \n",
      "13     Francesca Carleschi\n",
      "29                  Errato\n",
      "50             Ida Dâ€™avino\n",
      "201          Giulia Risina\n",
      "293      Isabel Cacciamali\n",
      "295          Elena Govetto\n",
      "Name: name, dtype: object\n",
      "Participant Name: Giulia Risina\n",
      "Participant ID: dcf792eb-d444-480f-b722-1ec4f0e82899\n",
      "Found kits: ['1']\n",
      "Number of samples retrieved: 12\n",
      "[{'barcode': 'M 005076 D', 'collectedAt': '2022-05-22T05:59:49.950Z'}, {'barcode': 'M 005069 D', 'collectedAt': '2022-05-07T06:09:33.882Z'}, {'barcode': 'M 005066 D', 'collectedAt': '2022-05-05T04:12:27.983Z'}, {'barcode': 'M 005074 D', 'collectedAt': '2022-05-12T06:02:28.023Z'}, {'barcode': 'M 005070 D', 'collectedAt': '2022-05-16T06:36:27.454Z'}, {'barcode': 'M 005075 D', 'collectedAt': '2022-05-20T07:08:13.424Z'}, {'barcode': 'M 005063 D', 'collectedAt': '2022-05-24T05:52:05.371Z'}, {'barcode': 'M 005043 D', 'collectedAt': '2022-07-12T07:46:28.247Z'}, {'barcode': 'M 005064 D', 'collectedAt': '2022-05-18T05:38:37.029Z'}, {'barcode': 'M 005062 D', 'collectedAt': '2022-05-14T05:17:39.060Z'}, {'barcode': 'M 005077 D', 'collectedAt': '2022-05-09T07:21:56.650Z'}, {'barcode': 'M 005041 D', 'collectedAt': '2022-07-05T09:14:41.765Z'}]\n",
      "Number of estradiol measurements retrieved: 10\n",
      "Number of progesterone measurements retrieved: 10\n",
      "collectedDate\n",
      "2022-05-05       NaN\n",
      "2022-05-06       NaN\n",
      "2022-05-07     2.868\n",
      "2022-05-08       NaN\n",
      "2022-05-09       NaN\n",
      "2022-05-10       NaN\n",
      "2022-05-11       NaN\n",
      "2022-05-12     2.657\n",
      "2022-05-13       NaN\n",
      "2022-05-14     4.723\n",
      "2022-05-15       NaN\n",
      "2022-05-16     4.747\n",
      "2022-05-17       NaN\n",
      "2022-05-18     5.537\n",
      "2022-05-19       NaN\n",
      "2022-05-20     6.706\n",
      "2022-05-21       NaN\n",
      "2022-05-22    11.066\n",
      "2022-05-23       NaN\n",
      "2022-05-24     3.338\n",
      "Freq: D, Name: edata, dtype: float64\n",
      "{\"0\":{\"key\":\"0\",\"questionTitle\":\"Your Food\",\"questionMode\":\"single\",\"value\":[\"Caffeinated drinks\",\"Meat\",\"Broccoli & other green veg\"],\"type\":\"multiple_choice_multi_select\",\"interval\":\"regular\",\"options\":[{\"label\":\"Caffeinated drinks\",\"imageOption\":\"coffee\"},{\"label\":\"Alcohol\",\"imageOption\":\"alcohol\"},{\"label\":\"Chocolate\",\"imageOption\":\"chocolate\"},{\"label\":\"Onions or garlic\",\"imageOption\":\"onion\"},{\"label\":\"Meat\",\"imageOption\":\"meat\"},{\"label\":\"Broccoli & other green veg\",\"imageOption\":\"veggie\"}],\"showChart\":\"no\"},\"1\":{\"key\":\"1\",\"questionTitle\":\"Your Weight\",\"questionMode\":\"single\",\"value\":\"No change\",\"type\":\"multiple_choice\",\"interval\":\"regular\",\"options\":[{\"label\":\"Lost weight\",\"imageOption\":null},{\"label\":\"No change\",\"imageOption\":null},{\"label\":\"Gained weight\",\"imageOption\":null}],\"showChart\":\"no\"},\"2\":{\"key\":\"2\",\"questionTitle\":\"Bleeding\",\"questionMode\":\"primary\",\"value\":\"No\",\"type\":\"multiple_choice\",\"interval\":\"regular\",\"options\":[{\"label\":\"Yes\",\"imageOption\":null},{\"label\":\"No\",\"imageOption\":null}],\"showChart\":\"no\"},\"4\":{\"key\":\"4\",\"questionTitle\":\"Rate your Symptoms\",\"questionMode\":\"single\",\"value\":[{\"label\":\"Backache\"},{\"label\":\"Joint pain\"},{\"label\":\"Abdominal pain\",\"value\":0},{\"label\":\"Abdominal cramps\",\"value\":0},{\"label\":\"Breast tenderness\"},{\"label\":\"Headaches\"},{\"label\":\"Heavy legs\"},{\"label\":\"Muscle spasms\"},{\"label\":\"Clumsiness\"},{\"label\":\"Gastrointestinal upset\"},{\"label\":\"Food craving\"},{\"label\":\"Binge eating\"},{\"label\":\"Reduced appetite\"},{\"label\":\"Fatigue\"},{\"label\":\"Increased sex drive\"},{\"label\":\"Skin problems\"},{\"label\":\"Common cold symptoms\"},{\"label\":\"Low mood\"},{\"label\":\"Sadness\"},{\"label\":\"Tearful\"},{\"label\":\"Mood swings\"},{\"label\":\"Anxiety\"},{\"label\":\"Paranoia\"},{\"label\":\"Concentration loss\"},{\"label\":\"Confusion\"}],\"type\":\"slider\",\"interval\":\"regular\",\"options\":[{\"label\":\"Backache\",\"imageOption\":\"\"},{\"label\":\"Joint pain\"},{\"label\":\"Abdominal pain\"},{\"label\":\"Abdominal cramps\"},{\"label\":\"Breast tenderness\"},{\"label\":\"Headaches\"},{\"label\":\"Heavy legs\"},{\"label\":\"Muscle spasms\"},{\"label\":\"Clumsiness\"},{\"label\":\"Gastrointestinal upset\"},{\"label\":\"Food craving\"},{\"label\":\"Binge eating\"},{\"label\":\"Reduced appetite\"},{\"label\":\"Fatigue\"},{\"label\":\"Increased sex drive\"},{\"label\":\"Skin problems\"},{\"label\":\"Common cold symptoms\"},{\"label\":\"Low mood\"},{\"label\":\"Sadness\"},{\"label\":\"Tearful\"},{\"label\":\"Mood swings\"},{\"label\":\"Anxiety\"},{\"label\":\"Paranoia\"},{\"label\":\"Concentration loss\"},{\"label\":\"Confusion\"}],\"showChart\":\"yes\"},\"5\":{\"key\":\"5\",\"questionTitle\":\"Your Sleep\",\"questionMode\":\"single\",\"value\":\"Great\",\"type\":\"multiple_choice\",\"interval\":\"regular\",\"options\":[{\"label\":\"Poor\",\"imageOption\":\"sheep\"},{\"label\":\"Restless\",\"imageOption\":\"moon\"},{\"label\":\"Good\",\"imageOption\":\"bed\"},{\"label\":\"Great\",\"imageOption\":\"alarm\"}],\"showChart\":\"yes\"},\"6\":{\"key\":\"6\",\"questionTitle\":\"Hormonal Contraception\",\"questionMode\":\"primary\",\"value\":\"No\",\"type\":\"multiple_choice\",\"interval\":\"monthly\",\"options\":[{\"label\":\"Yes\",\"imageOption\":null},{\"label\":\"No\",\"imageOption\":null}],\"showChart\":\"yes\"}}\n"
     ]
    }
   ],
   "source": [
    "## Gets data from AWS for chosen participant org, study, IDs\n",
    "## Splits data into ~weekly frequencies\n",
    "##determine follucular length from Bull paper\n",
    "##determine luteal length from Bull paper\n",
    "##determine baseline as median average of all valid measurements of P4 in follicular phase (around the first 8-12 days of cycle)\n",
    "##determine ovulatory threshold as 1.5x baseline\n",
    "##determine ovulatory day as the first day that P4 is above the ovulatory threshold - 4 days within luteal phase (follicular length:end of cycle)\n",
    "\n",
    "\n",
    "## CHANGE PATH BELOW AND RUN BLOCK\n",
    "## YOU WILL BE ASKED TO ENTER NAME OF ORG, STUDY AND PARTICIPANT NAME (OR BATCH ORG/STUDY)\n",
    "## SCRIPTS WILL OUTPUT OVULATION DATA IN CSV FILES AND PLOTS IN PNG FILES TO GENERATED FOLDERS SPECIFIC TO THE ORG/STUDY/PARTICIPANT\n",
    "\n",
    "## The data which collapses participant is also stored in a dataframe called perpardataframe, which can be used for further analysis \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Import necessary libraries\n",
    "\n",
    "import ov_detect\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import boto3\n",
    "import json\n",
    "import boto3.session\n",
    "# Import DynamoDB query and scan conditions\n",
    "from boto3.dynamodb.conditions import GreaterThan, Key, Attr\n",
    "import dynamo_pandas\n",
    "from dynamo_pandas import get_df, keys, put_df\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "import seaborn as sns\n",
    "\n",
    "# Define path for data processing and output\n",
    "## PLEASE DEFINE YOUR PATH HERE, ALL FOLDERS WILL BE CREATED INSIDE OF IT\n",
    "path = \"C:/Users/KatherineRidley/Mint Diagnostics Ltd/Hormone Data Analysis - General/Notebooks/OvDetect/\"\n",
    "\n",
    "# Initialize getdata instance to connect to DynamoDB\n",
    "get = ov_detect.getdata(path)\n",
    "dynamodb = get.connect_dynamodb(region = 'eu-west-2', profile = 'default')\n",
    "\n",
    "# Define DynamoDB tables\n",
    "table_athlete = dynamodb.Table('Athlete-qx7lirnxjfdzxoss6cmomxzgpe-staging')\n",
    "table_sample = dynamodb.Table('Sample-qx7lirnxjfdzxoss6cmomxzgpe-staging')\n",
    "table_measurement = dynamodb.Table('Measurement-qx7lirnxjfdzxoss6cmomxzgpe-staging')\n",
    "table_study = dynamodb.Table('Study-qx7lirnxjfdzxoss6cmomxzgpe-staging')\n",
    "table_testbatchcode = dynamodb.Table('TestBatchCode-qx7lirnxjfdzxoss6cmomxzgpe-staging')\n",
    "table_answer = dynamodb.Table('Answer-qx7lirnxjfdzxoss6cmomxzgpe-staging')\n",
    "table_organisation = dynamodb.Table('Organisation-qx7lirnxjfdzxoss6cmomxzgpe-staging')\n",
    "\n",
    "# Initialize list of dataframes for later concatenation\n",
    "\n",
    "listofdfs = []\n",
    "\n",
    "# Initialize list of per participant dataframes for later concatenation\n",
    "\n",
    "perpardfs = []\n",
    "\n",
    "# Initialize list of hazard dataframes for later concatenation\n",
    "\n",
    "hazarddfs = []\n",
    "\n",
    "# Retrieve organisation metadata\n",
    "\n",
    "org_df_meta=dynamo_pandas.get_df(table='Organisation-qx7lirnxjfdzxoss6cmomxzgpe-staging')\n",
    "print('Please input organisation name from this list: ')    \n",
    "print(org_df_meta.name)\n",
    "\n",
    "# Get organisation code from DynamoDB\n",
    "\n",
    "org_code_gcr=get.get_org_from_dynamo('Organisation-qx7lirnxjfdzxoss6cmomxzgpe-staging')\n",
    "\n",
    "# If org_code_gcr is a string, then it is a single organisation\n",
    "\n",
    "if isinstance(org_code_gcr, str):\n",
    "    org_code=org_code_gcr[:-4]\n",
    "    print('...')\n",
    "    inTitles = False\n",
    "    while inTitles == False:\n",
    "        print('Do you want to choose a programme from this organisation? (y/n)')\n",
    "        prog_yn = input('Do you want to choose a programme? (y/n): ')\n",
    "        if (prog_yn == 'y') | (prog_yn == 'Y'):\n",
    "            inTitles=True\n",
    "            prog_id= get.get_prog_from_org(org_code_gcr)\n",
    "            print('...')\n",
    "            par_id= get.get_participants_from_study(prog_id, org_code_gcr)\n",
    "            #batch_yn = input('Do you want to generate a batch report for all particpants in this programme? (y/n): ')\n",
    "        elif (prog_yn == 'n') | (prog_yn == 'N'):\n",
    "            par_id =get.get_participants_from_org(org_code_gcr)\n",
    "            inTitles =True\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# Initialize a dictionary with keys representing various attributes and values as empty lists\n",
    "\n",
    "\n",
    "perpardict = {'Participant':[], 'Cycle':[], 'Frequency':[], 'Organisation':[], 'Programme':[], 'RBOvulatory':[], 'P4Ovulatory':[], 'P4OvulatoryDay':[]}\n",
    "\n",
    "\n",
    "\n",
    "# Loop through each participant ID in the par_id list\n",
    "\n",
    "\n",
    "for par in par_id:\n",
    "\n",
    "    hazards = {'Participant':[], 'Cycle':[], 'Frequency':[], 'Organisation':[], 'Programme':[], 'HazardCode':[], 'Hazard':[]} # Reset hazards dictionary for each participant\n",
    "\n",
    "    # Get the test batch codes for estradiol and progesterone\n",
    "\n",
    "\n",
    "    estradiol_tbc_id, progesterone_tbc_id = get.get_test_batch_codes('TestBatchCode-qx7lirnxjfdzxoss6cmomxzgpe-staging')\n",
    "    \n",
    "    # Get the list of all kit numbers for the current participant\n",
    "\n",
    "\n",
    "    kitlist = get.getallkitnums(par, get.scan_table(table_measurement, filterExp = {'measurementAthleteId' : par}, expAttrNames = {'#i' : 'id', '#b' : 'barcode', '#m' : 'measurementAthleteId' , '#v' : 'value', '#k' : 'kit', '#c' : 'collectedAt', '#t':'measurementTestBatchCodeId'}))\n",
    "    \n",
    "    # Check if there are any kit numbers for the participant\n",
    "\n",
    "\n",
    "    if len(kitlist) > 0:\n",
    "\n",
    "        # Loop through each kit number in the kitlist\n",
    "\n",
    "\n",
    "        for kitnum in kitlist:\n",
    "\n",
    "            # Get the samples data for the current participant and kit number\n",
    "\n",
    "\n",
    "            samples_df = get.get_samples(get.scan_table(dynamoTable = table_sample, filterExp = {'sampleAthleteId' : par, 'kit' : kitnum}, expAttrNames = {'#b' : 'barcode', '#c' : 'collectedAt'}))\n",
    "\n",
    "            # Check if there are any samples in the samples_df\n",
    "\n",
    "            if len(samples_df) > 0:\n",
    "\n",
    "                # Get the estradiol (e_df) and progesterone (p_df) data for the current participant and kit number\n",
    "\n",
    "\n",
    "                e_df, p_df = get.get_e_df_and_p_df(estradiol_tbc_id, progesterone_tbc_id, get.scan_table(table_measurement,\n",
    "                                        filterExp = {'measurementAthleteId' : par,  'kit' : kitnum}, expAttrNames = {'#i' : 'id', \n",
    "                                                    '#b' : 'barcode' , \n",
    "                                                    '#v' : 'value', \n",
    "                                                    '#k' : 'kit', \n",
    "                                                    '#c' : 'collectedAt',\n",
    "                                                    '#t':'measurementTestBatchCodeId', '#s':'studyID'}))\n",
    "                # Check if either e_df or p_df is empty\n",
    "\n",
    "                print(e_df['edata'])\n",
    "\n",
    "                if (len(e_df) == 0) | (len(p_df) == 0):\n",
    "                    continue\n",
    "\n",
    "                else:\n",
    "                    \n",
    "                    # Get the study ID from the progesterone data\n",
    "                    progid = p_df['studyID'].iloc[0]\n",
    "                    # Get the study data based on the organization code and study ID\n",
    "                    study = get.scan_table(table_study,\n",
    "                                            filterExp = {'groupCanRead' : org_code_gcr, 'id': progid}, expAttrNames = {'#i' : 'id', \n",
    "                                                        '#t' : 'title'})\n",
    "                    \n",
    "                    # Convert the study data to a dataframe\n",
    "                    study = pd.DataFrame.from_dict(study)\n",
    "\n",
    "                    # Get the program title from the study dataframe\n",
    "                    progid = study['title'].iloc[0]\n",
    "                \n",
    "                    # Get the answers data for the current participant and kit number\n",
    "                    \n",
    "\n",
    "                    \n",
    "                    answers_df=get.get_answers(get.scan_table(dynamoTable = table_answer,\n",
    "                                            filterExp = {'athleteID' : par,  'kit' : kitnum},\n",
    "                                            expAttrNames = {'#v' : 'value', '#c' : 'collectedAt'}))\n",
    "                    \n",
    "                    print(answers_df)\n",
    "                    \n",
    "                    '''\n",
    "                        \n",
    "                        \n",
    "                    # Merge the participant data, kit number, estradiol, progesterone, and answers data into a single dataframe\n",
    "                    df = get.aws_data_merge(par, kitnum, e_df, p_df, answers_df)\n",
    "\n",
    "                    # Create a new dataframe with only the 'collectedDate' and 'P4' columns\n",
    "                    newdf = df[['collectedDate', 'P4']]\n",
    "\n",
    "                    # Define the output directories for saving the results\n",
    "                    output = os.path.join(path, org_code_gcr, 'prog_{}'.format(progid))\n",
    "                    output2 = os.path.join(output, par)\n",
    "                    output3 = os.path.join(output, 'Without_ovulation_markers')\n",
    "                    output4 = os.path.join(output2, kitnum[-1])\n",
    "\n",
    "                    # Create the output directories if they don't exist\n",
    "                    os.makedirs(output, exist_ok=True)\n",
    "\n",
    "                    if not os.path.exists(output+'/'+par+'/'):\n",
    "                        os.mkdir(output+'/'+par+'/')\n",
    "\n",
    "                    if not os.path.exists(output3):\n",
    "                        os.mkdir(output3)\n",
    "\n",
    "                    if not os.path.exists(output4):\n",
    "                        os.mkdir(output4)\n",
    "\n",
    "                    # Create an instance of the DataHandling class with the merged dataframe and output directory\n",
    "                    handler = ov_detect.datahandling(df, output)\n",
    "\n",
    "                    # Check if the length of the dataframe is too long or too short, and skip the current iteration if necessary\n",
    "\n",
    "                    #start defining and outputting hazards for the participant\n",
    "                    if len(df) > 35:\n",
    "                        print('Cycle too long, skipping...')\n",
    "                        hazards = handler.hazards(par, kitnum, 'Original', org_code_gcr, progid, hazards, '6', 'Cycle too long, skipping...')\n",
    "                        \n",
    "                        \n",
    "\n",
    "                    elif len(df) < 21:\n",
    "                        print('Cycle too short, skipping...')\n",
    "                        hazards = handler.hazards(par, kitnum, 'Original', org_code_gcr, progid, hazards, '6', 'Cycle too short, skipping...')\n",
    "\n",
    "                    if len(df.loc[df['Menses']==1]) == 0:\n",
    "                        hazards = handler.hazards(par, kitnum, 'Original', org_code_gcr, progid, hazards, '4', 'No menses info')\n",
    "\n",
    "                    if len(df.loc[df['P4']>0, 'P4']) < 4:\n",
    "                        hazards = handler.hazards(par, kitnum, 'Original', org_code_gcr, progid, hazards, '1', 'Not enough P4 data, skipping...')\n",
    "\n",
    "                        \n",
    "\n",
    "                    # Check if the length of the dataframe is within the valid range\n",
    "                    if (len(df) > 21) & (len(df) < 35):\n",
    "                        # Determine the starting date based on the 'P4' column\n",
    "                        if df.loc[df['P4'].notnull(), 'P4'].values[0] < 60:\n",
    "                            starting_date = df.loc[df['P4'].notnull(), 'collectedDate'].values[0]\n",
    "                        else:\n",
    "                            starting_date = df.loc[df['P4'].notnull(), 'collectedDate'].values[1]\n",
    "\n",
    "                        # Initialize an empty list for storing selected dates\n",
    "                        selected_dates = []\n",
    "                        max_date = df['collectedDate'].max()\n",
    "\n",
    "                        # Loop through the dates with a one-week interval\n",
    "                        while starting_date <= (max_date + pd.to_timedelta(4, unit='D')):\n",
    "                            closest_date = handler.closest_date_with_p4(df, starting_date)\n",
    "                            if closest_date is not None:\n",
    "                                selected_dates.append(closest_date)\n",
    "                                starting_date == (closest_date + pd.to_timedelta(7, unit='D'))\n",
    "\n",
    "                            # Increment the starting date by one week\n",
    "                            starting_date += pd.to_timedelta(1, unit='W')\n",
    "\n",
    "                        # Append the last date if it's more than 5 days away from the previous date\n",
    "                        if selected_dates[-1] < max_date - pd.to_timedelta(5, unit='D'):\n",
    "                            selected_dates.append(df.loc[df['P4'].notnull(), 'collectedDate'].max())\n",
    "\n",
    "                        # Create a copy of the dataframe and update the 'collectedDate' and 'P4' columns based on the selected_dates list\n",
    "                        output_df = df.copy()\n",
    "                        output_df['collectedDate'] = output_df['collectedDate']\n",
    "                        output_df.loc[~output_df['collectedDate'].isin(selected_dates), 'P4'] = None\n",
    "                        output_df.loc[~output_df['collectedDate'].isin(selected_dates), 'E2'] = None\n",
    "\n",
    "                        # Create a copy of the output_df and interpolate the 'P4' and 'E2' columns using linear interpolation\n",
    "                        weekly_df = output_df.copy()\n",
    "    \n",
    "\n",
    "                        weekly_df['P4_linear'] = weekly_df['P4'].interpolate(method='linear')\n",
    "                        weekly_df['P4_linear'] = weekly_df['P4_linear'].fillna(method='ffill')\n",
    "                    \n",
    "                        # Calculate rolling average with a window\n",
    "                        window = 3\n",
    "                        weekly_df['P4_rolling'] = weekly_df['P4_linear'].rolling(window).sum()/window\n",
    "                        weekly_df['P4_rolling'] = weekly_df['P4_rolling'].shift(periods = -1)\n",
    "\n",
    "                        weekly_df['E2_linear'] = weekly_df['E2'].interpolate(method='linear')\n",
    "                        weekly_df['E2_linear'] = weekly_df['E2_linear'].fillna(method='ffill')\n",
    "\n",
    "                        # Calculate rolling average with a window\n",
    "\n",
    "                        weekly_df['E2_rolling'] = weekly_df['E2_linear'].rolling(window).sum()/window\n",
    "                        weekly_df['E2_rolling'] = weekly_df['E2_rolling'].shift(periods = -1)\n",
    "\n",
    "                        df['Frequency'] = 'Original'\n",
    "\n",
    "                        weekly_df['Frequency'] = 'Weekly'\n",
    "\n",
    "                        ##weekly df interpolate\n",
    "\n",
    "                        # Interpolate the weekly_df dataframe\n",
    "                        # ...\n",
    "\n",
    "                        # Create a dictionary containing 'Original' and 'Weekly' dataframes\n",
    "                        freq_dict = {'Original': df, 'Weekly': weekly_df}\n",
    "\n",
    "                        # Iterate through the frequency dictionary and process each dataframe\n",
    "                        for freq, freq_df in freq_dict.items():\n",
    "                            # Initialize a detector object with the current frequency dataframe and output directory\n",
    "                            detector = ov_detect.detection(freq_df, output)\n",
    "\n",
    "                            # Calculate the baseline for the current participant, kit, and frequency\n",
    "                            print('Baseline calculation for participant: ' + par + ' kit: ' + kitnum[-1], 'frequency: ' + freq)\n",
    "\n",
    "                            # Determine the follicular length based on the cycle length\n",
    "                            cycle_length = len(df)\n",
    "                            if cycle_length <= 21:\n",
    "                                follicular_length = 8\n",
    "                                \n",
    "                            elif (cycle_length > 20) & (cycle_length <= 24):\n",
    "                                follicular_length = 10\n",
    "                            elif (cycle_length > 24) & (cycle_length <= 30):\n",
    "                                follicular_length = 13\n",
    "                            elif (cycle_length > 30) & (cycle_length <= 35):\n",
    "                                follicular_length = 16\n",
    "                            elif (cycle_length > 35):\n",
    "                                follicular_length = 18\n",
    "                                \n",
    "                            #if menses == 1 index outside of follicular phase index\n",
    "\n",
    "                            if (len(df.loc[df['Menses']==1]) > 0) & (len(df.loc[df['P4']>0]) > 4):\n",
    "\n",
    "                                if (df.loc[df['Menses']==1].index[0] > follicular_length):\n",
    "                                    hazards = handler.hazards(par, kitnum, freq, org_code_gcr, progid, hazards, '5', 'Menses detected outside of follicular phase, cycle may not have been detected correctly')\n",
    "\n",
    "                            # if no p4 value in follicular phase\n",
    "                            if len(df.loc[df['P4']>0, 'P4'][:follicular_length+1]) == 0:\n",
    "                                hazards = handler.hazards(par, kitnum, freq, org_code_gcr, progid, hazards, '3', 'No P4 sample in the follicular phase')\n",
    "\n",
    "                            # if p4 max in follicular > 90\n",
    "                            if df.loc[df['P4']>0, 'P4'][:follicular_length].max() > 90:\n",
    "                                hazards = handler.hazards(par, kitnum, freq, org_code_gcr, progid, hazards, '8', 'P4 max in follicular phase > 90')\n",
    "\n",
    "                            #luteal p4 max > 400                           \n",
    "                            if df.loc[df['P4']>0, 'P4'][follicular_length:].max() > 400:\n",
    "                                hazards = handler.hazards(par, kitnum, freq, org_code_gcr, progid, hazards, '7', 'P4 max in luteal phase > 400')\n",
    "\n",
    "                            #all p4 values > 90\n",
    "\n",
    "                            if df.loc[df['P4']>0, 'P4'].min() > 90:\n",
    "                                hazards = handler.hazards(par, kitnum, freq, org_code_gcr, progid, hazards, '7', 'All p4 values are above 90')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                            \n",
    "\n",
    "                            # Calculate the baseline for each column in the ['E2_linear', 'P4_linear'] list\n",
    "                            for column in ['E2_linear', 'P4_linear']:\n",
    "                                basedf = detector.baseline(par, kitnum[-1], column, follicular_length)\n",
    "\n",
    "                            # Detect ovulation index based on the progesterone level\n",
    "                            ovindex = detector.p4_ov_detect(basedf, follicular_length)\n",
    "\n",
    "                            # Initialize 'P4_ov_detected' and 'P4_ovulation_day' columns in basedf\n",
    "                            basedf['P4_ov_detected'] = 0\n",
    "                            basedf['P4_ovulation_day'] = 0\n",
    "\n",
    "                            # Update 'P4_ov_detected' column if ovulation is detected\n",
    "                            if ovindex != None:\n",
    "                                basedf.loc[ovindex, 'P4_ov_detected'] = 1\n",
    "\n",
    "                            # Detect ovulation day based on the progesterone level\n",
    "                            ovdayindex = detector.p4_ov_day(basedf, follicular_length)\n",
    "\n",
    "                            # Update 'P4_ovulation_day' column if ovulation day is detected\n",
    "                            if ovdayindex != None:\n",
    "                                basedf.loc[ovdayindex, 'P4_ovulation_day'] = 1\n",
    "\n",
    "                            # Calculate the E2/P4 ratios for both linear and original data\n",
    "                            basedf['E2/P4_linear'] = (basedf['E2_linear']/basedf['P4_linear'])*100\n",
    "                            basedf['E2/P4_linear'] = basedf['E2/P4_linear'].round(2)\n",
    "                            basedf['E2/P4'] = (basedf['E2']/basedf['P4'])*100\n",
    "                            basedf['E2/P4'] = basedf['E2/P4'].round(2)\n",
    "\n",
    "                            rangethreshold = 5\n",
    "\n",
    "                            # (The commented-out code for detecting ovulation based on E2/P4 ratio is not included in this annotation)\n",
    "\n",
    "                            # Plot the cycle with and without ovulation markers\n",
    "                            detector.plotcycle(basedf, org_code_gcr, par, kitnum[-1], freq, follicular_length)\n",
    "                            detector.plotcycle_no_ov(basedf, org_code_gcr, par, kitnum[-1], freq)\n",
    "\n",
    "                            # Sort the final dataframe by 'collectedDate' and save it as a CSV file\n",
    "                            finaldf = basedf.sort_values(by='collectedDate')\n",
    "                            \n",
    "\n",
    "                            #print(finaldf)\n",
    "                            finaldf.to_csv(output2+'/dataframe_{}_{}_{}.csv'.format(par, kitnum[-1], freq))\n",
    "\n",
    "\n",
    "                            # Append relevant information to the perpardict dictionary\n",
    "                            perpardict['Participant'].append(par)\n",
    "                            perpardict['Cycle'].append(kitnum[-1])\n",
    "                            perpardict['Frequency'].append(freq)\n",
    "                            perpardict['Organisation'].append(org_code_gcr)\n",
    "                            perpardict['Programme'].append(progid)\n",
    "                            perpardict['RBOvulatory'].append(0)\n",
    "\n",
    "                            # Update the P4Ovulatory and P4OvulatoryDay fields based on the ovulation index\n",
    "                            if ovindex != -1:\n",
    "                                perpardict['P4Ovulatory'].append(1)\n",
    "                                perpardict['P4OvulatoryDay'].append(ovdayindex)\n",
    "                            else:\n",
    "                                perpardict['P4Ovulatory'].append(0)\n",
    "                                perpardict['P4OvulatoryDay'].append(0)\n",
    "\n",
    "                            # Convert the perpardict dictionary to a DataFrame and save it as a CSV file\n",
    "                            perpardf = pd.DataFrame(perpardict)\n",
    "                            perpardf.to_csv(output4+'/perpardf_{}_{}.csv'.format(org_code_gcr, freq))\n",
    "\n",
    "                            hazardsdf = pd.DataFrame(hazards)\n",
    "                            hazardsdf.to_csv(output4+'/hazards_{}_{}_{}.csv'.format(par, kitnum[-1], freq))\n",
    "\n",
    "                            # Append the perpardf and finaldf dataframes to their respective lists\n",
    "                            perpardfs.append(perpardf)\n",
    "                            listofdfs.append(finaldf)\n",
    "                            hazarddfs.append(hazardsdf)\n",
    "\n",
    "# Concatenate all dataframes in the listofdfs and perpardfs lists\n",
    "fulldf = pd.concat(listofdfs)\n",
    "perpar = pd.concat(perpardfs)\n",
    "hazarddf = pd.concat(hazarddfs)\n",
    "\n",
    "# Remove duplicates from the perpar dataframe\n",
    "perpar = perpar.drop_duplicates(subset=['Participant', 'Cycle', 'Frequency'], keep='first')\n",
    "hazarddf = hazarddf.drop_duplicates(subset=['Participant', 'Cycle', 'Frequency'], keep='first')\n",
    "\n",
    "# Define the path for the merged data\n",
    "path_to_merge = path+\"perpar/mergefolder\"\n",
    "\n",
    "#create the mergefolder if it doesn't exist\n",
    "if not os.path.exists(path_to_merge):\n",
    "    os.makedirs(path_to_merge)\n",
    "\n",
    "# Save the fulldf and perpar dataframes as CSV files\n",
    "fulldf.to_csv(output+'/fulldf_{}.csv'.format(org_code))\n",
    "#fulldf.to_csv(path_to_merge+'/fulldf_{}.csv'.format(org_code))\n",
    "print(fulldf)\n",
    "\n",
    "hazarddf.to_csv(output+'/hazarddf_{}.csv'.format(org_code))\n",
    "\n",
    "perpar.to_csv(output+'/perpardataframe_{}.csv'.format(org_code))\n",
    "perpar.to_csv(path_to_merge+'/perpardataframe_{}.csv'.format(org_code))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#data = pd.read_csv(\"C:/Users/KatherineRidley/OneDrive - Mint Diagnostics Ltd/Documents/Data Analysis/KR_MNC_CYCLES.csv\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def validate_ovulation_detection(csv_directory, rb_key_path):\n",
    "    # Find all CSV files in the specified directory\n",
    "    all_files = glob.glob(os.path.join(csv_directory, \"*.csv\"))\n",
    "    \n",
    "    # Filter out files with 'perpardataframe_' in their name\n",
    "    all_files = [x for x in all_files if 'perpardataframe_' in x]\n",
    "\n",
    "    li = []\n",
    "\n",
    "    # Read and append each CSV file to a list\n",
    "    for filename in all_files:\n",
    "        df = pd.read_csv(filename, index_col=None, header=0)\n",
    "        li.append(df)\n",
    "\n",
    "    # Concatenate all DataFrames in the list\n",
    "    frame = pd.concat(li, axis=0, ignore_index=True)\n",
    "    \n",
    "    # Save the merged DataFrame to a CSV file\n",
    "    frame.to_csv(csv_directory + '/perpardataframe4.csv')\n",
    "\n",
    "    # Read the RB_key CSV file\n",
    "    dfkey = pd.read_csv(rb_key_path)\n",
    "    \n",
    "    # Merge the 'frame' DataFrame with the 'dfkey' DataFrame\n",
    "    df_merged = pd.merge(frame, dfkey[['Participant', 'Cycle', 'Frequency', 'RBOvulatory']], on=['Participant', 'Cycle', 'Frequency'], how='left')\n",
    "    \n",
    "    # Drop duplicates\n",
    "    df_merged = df_merged.drop_duplicates(subset=['Participant', 'Cycle', 'Frequency'], keep='first')\n",
    "    \n",
    "    # Save the merged DataFrame to a CSV file\n",
    "    df_merged.to_csv(csv_directory + '/perpardataframe5.csv')\n",
    "\n",
    "    # Read the merged CSV file\n",
    "    frame = pd.read_csv(csv_directory + '/perpardataframe5.csv')\n",
    "    \n",
    "    # Initialize an empty dictionary to store metrics results\n",
    "    metrics_results = {}\n",
    "\n",
    "    # Loop through each unique frequency in the 'frame' DataFrame\n",
    "    for f in frame.Frequency.unique():\n",
    "        # Filter the DataFrame by frequency\n",
    "        df = frame[frame['Frequency'] == f]\n",
    "        \n",
    "        # Calculate performance metrics\n",
    "        TP = len(df[(df['RBOvulatory'] == 1) & (df['P4Ovulatory'] == 1)])\n",
    "        FP = len(df[(df['RBOvulatory'] == 0) & (df['P4Ovulatory'] == 1)])\n",
    "        TN = len(df[(df['RBOvulatory'] == 0) & (df['P4Ovulatory'] == 0)])\n",
    "        FN = len(df[(df['RBOvulatory'] == 1) & (df['P4Ovulatory'] == 0)])\n",
    "\n",
    "        # Create a dictionary of metrics\n",
    "        metrics = {'True Positive': TP, 'False Positive': FP, 'True Negative': TN, 'False Negative': FN, 'Precision': TP/(TP+FP), 'Recall': TP/(TP+FN), 'Accuracy': (TP+TN)/(TP+TN+FP+FN), 'F1 Score': 2*(TP/(TP+FP))*(TP/(TP+FN))/((TP/(TP+FP))+(TP/(TP+FN))), 'Specificity': TN/(TN+FP)}\n",
    "\n",
    "        # Add the frequency-wise metrics to the metrics_results dictionary\n",
    "        metrics_results[f] = metrics\n",
    "        \n",
    "        # Convert the metrics dictionary to a DataFrame\n",
    "        metricsdf = pd.DataFrame(metrics, index=[0])\n",
    "        \n",
    "        # Save the metrics DataFrame to a CSV file\n",
    "        metricsdf.to_csv(csv_directory + '/metrics_{}.csv'.format(f))\n",
    "\n",
    "    return metrics_results\n",
    "\n",
    "# Example usage\n",
    "csv_directory = \"C:/Users/KatherineRidley/Mint Diagnostics Ltd/Hormone Data Analysis - General/Notebooks/OvDetect/perpar/mergefolder\"\n",
    "rb_key_path = \"C:/Users/KatherineRidley/Mint Diagnostics Ltd/Hormone Data Analysis - General/Notebooks/OvDetect/perpar/mergefolder/RB_key.csv\"\n",
    "\n",
    "\n",
    "metrics_results = validate_ovulation_detection(csv_directory, rb_key_path)\n",
    "print(metrics_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Validating ovulation detection\n",
    "# Merges all CSV files that have been previously annotated by RB in a directory into a single CSV file\n",
    "# Takes RBs annotations from RB_key.csv and adds them to the merged CSV file\n",
    "# calculates metrics for each cycle and saves them to a CSV file\n",
    "\n",
    "\n",
    "import glob\n",
    "\n",
    "# Set the path to the directory containing the CSV files\n",
    "path = \"C:/Users/KatherineRidley/Mint Diagnostics Ltd/Hormone Data Analysis - General/Notebooks/OvDetect/perpar/mergefolder\"\n",
    "\n",
    "# Find all CSV files in the directory\n",
    "all_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "\n",
    "# Filter out files with 'perpardataframe_' in their name\n",
    "all_files = [x for x in all_files if 'perpardataframe_' in x]\n",
    "\n",
    "li = []\n",
    "\n",
    "# Read and append each CSV file to a list\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    li.append(df)\n",
    "\n",
    "# Concatenate all DataFrames in the list\n",
    "frame = pd.concat(li, axis=0, ignore_index=True)\n",
    "\n",
    "# Save the merged DataFrame to a CSV file\n",
    "frame.to_csv(path+'/perpardataframe4.csv')\n",
    "\n",
    "# Read the RB_key CSV file\n",
    "dfkey = pd.read_csv(\"C:/Users/KatherineRidley/Mint Diagnostics Ltd/Hormone Data Analysis - General/Notebooks/OvDetect/perpar/mergefolder/RB_key.csv\")\n",
    "\n",
    "# Merge the 'frame' DataFrame with the 'dfkey' DataFrame\n",
    "df_merged = pd.merge(frame, dfkey[['Participant', 'Cycle', 'Frequency', 'RBOvulatory']], on=['Participant', 'Cycle', 'Frequency'], how='left')\n",
    "\n",
    "# Drop duplicates\n",
    "df_merged = df_merged.drop_duplicates(subset=['Participant', 'Cycle', 'Frequency'], keep='first')\n",
    "\n",
    "# Save the merged DataFrame to a CSV file\n",
    "df_merged.to_csv(path+'/perpardataframe5.csv')\n",
    "\n",
    "# Read the merged CSV file\n",
    "frame = pd.read_csv(path+'/perpardataframe5.csv')\n",
    "\n",
    "# Loop through each unique frequency in the 'frame' DataFrame\n",
    "for f in frame.Frequency.unique():\n",
    "\n",
    "    # Filter the DataFrame by frequency\n",
    "    df = frame[frame['Frequency'] == f]\n",
    "\n",
    "    # Calculate performance metrics\n",
    "    TP = len(df[(df['RBOvulatory'] == 1) & (df['P4Ovulatory'] == 1)])\n",
    "    FP = len(df[(df['RBOvulatory'] == 0) & (df['P4Ovulatory'] == 1)])\n",
    "    TN = len(df[(df['RBOvulatory'] == 0) & (df['P4Ovulatory'] == 0)])\n",
    "    FN = len(df[(df['RBOvulatory'] == 1) & (df['P4Ovulatory'] == 0)])\n",
    "\n",
    "    metrics = {'True Positive': TP, 'False Positive': FP, 'True Negative': TN, 'False Negative': FN, 'Precision': TP/(TP+FP), 'Recall': TP/(TP+FN), 'Accuracy': (TP+TN)/(TP+TN+FP+FN), 'F1 Score': 2*(TP/(TP+FP))*(TP/(TP+FN))/((TP/(TP+FP))+(TP/(TP+FN))), 'Specificity': TN/(TN+FP)}\n",
    "\n",
    "    # Convert the metrics dictionary to a DataFrame\n",
    "    metricsdf = pd.DataFrame(metrics, index=[0])\n",
    "\n",
    "    # Save the metrics DataFrame to a CSV file\n",
    "    metricsdf.to_csv(path+'/metrics_{}.csv'.format(f))\n",
    "\n",
    "    # Print the metrics DataFrame\n",
    "    print(metricsdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MAN CITY OVULATION\n",
    "\n",
    "import ov_detect\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import boto3\n",
    "import json\n",
    "import boto3.session\n",
    "# https://boto3.amazonaws.com/v1/documentation/api/latest/guide/dynamodb.html#querying-and-scanning\n",
    "from boto3.dynamodb.conditions import GreaterThan, Key, Attr\n",
    "import dynamo_pandas\n",
    "from dynamo_pandas import get_df, keys, put_df\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"C:/Users/KatherineRidley/OneDrive - Mint Diagnostics Ltd/Documents/Data Analysis/KR_MNC_CYCLES.csv\")\n",
    "\n",
    "#print(data.head())\n",
    "\n",
    "\n",
    "\n",
    "for i in data['Player'].unique():\n",
    "\n",
    "    for j in data['Cycle '].unique():\n",
    "\n",
    "        indata = data[(data['Player'] == i) & (data['Cycle '] == j)]\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        if len(indata) >0:\n",
    "\n",
    "            \n",
    "\n",
    "            #print(data.loc[(data['Player'] == i) & (data['Cycle '] == j), 'Date'])\n",
    "\n",
    "            \n",
    "\n",
    "             \n",
    "\n",
    "data = data.fillna(0)\n",
    "\n",
    "\n",
    "data1perweek = data[data['Every 7th day']==1]\n",
    "dataeveryother = data[data['Every other day']==1]\n",
    "dataevery3rd = data[data['Every 3rd day']==1]\n",
    "datetwiceaweek = data[data['Twice a week']==1]\n",
    "\n",
    "data1perweek.to_csv('C:/Users/KatherineRidley/OneDrive - Mint Diagnostics Ltd/Documents/Data Analysis/MNC_1perweek.csv')\n",
    "dataeveryother.to_csv('C:/Users/KatherineRidley/OneDrive - Mint Diagnostics Ltd/Documents/Data Analysis/MNC_everyother.csv')\n",
    "dataevery3rd.to_csv('C:/Users/KatherineRidley/OneDrive - Mint Diagnostics Ltd/Documents/Data Analysis/MNC_every3rd.csv')\n",
    "datetwiceaweek.to_csv('C:/Users/KatherineRidley/OneDrive - Mint Diagnostics Ltd/Documents/Data Analysis/MNC_twiceaweek.csv')\n",
    "data.to_csv('C:/Users/KatherineRidley/OneDrive - Mint Diagnostics Ltd/Documents/Data Analysis/full.csv')\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "           #print(d['d'].loc[(d['d']['Player'] == i) & (d['d']['Cycle '] == j), 'E2_difference'])\n",
    "\n",
    "\n",
    "#print(data)\n",
    "\n",
    "##kassam \n",
    "\n",
    "##save all data to csv\n",
    "\n",
    "def replace_with_NaN(df, col):\n",
    "    df2 = df.copy()\n",
    "    columns = ['E2', 'P4', 'E2_difference', 'P4_difference']\n",
    "    for column in columns:\n",
    "        \n",
    "        df2[column] = np.where(df2[col] != 1, np.nan, df2[column])\n",
    "    return df2\n",
    "\n",
    "#print(data)\n",
    "\n",
    "cols = ['Every 7th day', 'Every other day', 'Every 3rd day', 'Twice a week']\n",
    "\n",
    "every3rdfull = replace_with_NaN(data, 'Every 3rd day')\n",
    "everyotherfull = replace_with_NaN(data, 'Every other day')\n",
    "twiceaweekfull = replace_with_NaN(data, 'Twice a week')\n",
    "oneperweekfull = replace_with_NaN(data, 'Every 7th day')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cols = ['Every 7th day', 'Every other day', 'Every 3rd day', 'Twice a week']\n",
    "\n",
    "every3rdfull = replace_with_NaN(data, 'Every 3rd day')\n",
    "everyotherfull = replace_with_NaN(data, 'Every other day')\n",
    "twiceaweekfull = replace_with_NaN(data, 'Twice a week')\n",
    "oneperweekfull = replace_with_NaN(data, 'Every 7th day')\n",
    "\n",
    "\n",
    " \n",
    "dataall = {'d': data, 'baseline': 5, 'kassam':1, 'baird':5, 'name': 'All'}\n",
    "everyother = {'d': everyotherfull, 'baseline': 3, 'kassam':2, 'baird':3, 'name': 'Every other day'}\n",
    "every3rd = {'d': every3rdfull, 'baseline': 1, 'kassam':1, 'baird':3, 'name': 'Every 3rd day'}\n",
    "twiceaweek = {'d': twiceaweekfull, 'baseline': 1, 'kassam':1, 'baird':3, 'name': 'Twice a week'}\n",
    "oneperweek = {'d': oneperweekfull, 'baseline': 1, 'kassam':1, 'baird':2, 'name': 'Every 7th day'}\n",
    "\n",
    "\n",
    "\n",
    "for d in [dataall, every3rd, oneperweek]:\n",
    "    dt = d['d']\n",
    "    base = d['baseline']\n",
    "\n",
    "    for i in dt['Player'].unique():\n",
    "        for j in dt['Cycle '].unique():\n",
    "            baseline(dt, i, j, 'E2', base)\n",
    "            baseline(dt, i, j, 'P4', base)\n",
    "\n",
    "\n",
    "    idxlist = []\n",
    "    dt = d['d']\n",
    "    kass = d['kassam']\n",
    "    print(d['name'])\n",
    "    #print(idxlist)\n",
    "    #print(dt.index.to_list())\n",
    "    for i in dt['Player'].unique():\n",
    "        for j in dt['Cycle '].unique():\n",
    "            print(i, j)\n",
    "            idxlist.append(kassam(dt, i, j, kass))\n",
    "\n",
    "    idxlist = [idx for idx in idxlist if (idx != None) & (idx != -6) & (idx != -2) & (idx != -4) & (idx != -1)]\n",
    "    print(idxlist)\n",
    "    dt.loc[idxlist, 'Kassam'] = 1\n",
    "    dt['Kassam'] = dt['Kassam'].fillna(0)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for d in [dataall, everyother, every3rd, twiceaweek, oneperweek]:\n",
    "    idxlist = []\n",
    "    dt = d['d']\n",
    "    idxlist = []\n",
    "    print(d['name'])\n",
    "    for i in dt['Player'].unique():\n",
    "        for j in dt['Cycle '].unique():\n",
    "\n",
    "            #print(i, j)\n",
    "            \n",
    "            idxlist.append(baird(dt, i, j, d['baird']))\n",
    "\n",
    "    idxlist = [idx for idx in idxlist if (idx != None) & (idx != -6)]\n",
    "    print(idxlist)\n",
    "    dt.loc[idxlist, 'Baird'] = 1\n",
    "    dt['Baird'] = dt['Baird'].fillna(0)\n",
    "\n",
    "\n",
    "datadict ={'every3rd':every3rd['d'], 'everyother':everyother['d'], 'oneperweek':oneperweek['d'], 'full':dataall['d']}\n",
    "\n",
    "#print(every3rd['d'].tail(20))\n",
    "#print(every3rd['d']['P4_difference'].tail(20))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "datadict ={'every 3rd day':every3rd['d'], 'one per week':oneperweek['d'], 'full':dataall['d']}\n",
    "\n",
    "'''for k, v in datadict.items():\n",
    "        for p in v.Player.unique():\n",
    "             for c in v['Cycle '].unique():\n",
    "                value = v.loc[(v['Player']==p) & (v['Cycle ']==c)]\n",
    "                if len(value) > 5:\n",
    "                    #value = value.set_index('Day')\n",
    "                    #value = value[['P4', 'E2', 'Menses']]\n",
    "                    value = value.replace(0, np.nan)\n",
    "                    values = value.dropna(subset=['P4', 'E2'])\n",
    "                    #print(value)\n",
    "                    #value= value.interpolate(method='linear', limit_direction='forward', axis=0)\n",
    "\n",
    "                    fig, (ax1) = plt.subplots(nrows=1, ncols=1, sharex = True, figsize=(10,5))\n",
    "                    ax2 = ax1.twinx()\n",
    "                    \n",
    "                    values.reset_index().plot(ax = ax1, x='Day', y='P4', color = 'palegreen', linewidth=6, label='Progesterone Average')\n",
    "                    values.reset_index().plot(ax = ax1, x='Day', y='P4', color = 'limegreen', marker = 'o', markersize = 6, linestyle = 'none', label = 'Progesterone')\n",
    "                    values.reset_index().plot(ax = ax2, x='Day', y='E2', color = '#FFC8D3', linewidth=6, label='Oestradiol Average')\n",
    "                    values.reset_index().plot(ax = ax2, x='Day', y='E2', color = '#DB2525', marker = 'o', markersize = 6, linestyle = 'none', label = 'Oestradiol')\n",
    "                    ylow, yhigh = ax1.get_ylim()\n",
    "                    xlow, xhigh = ax1.get_xlim()\n",
    "                    for i in range(len(value.loc[value['Menses']==1])):\n",
    "                        if i == 0:\n",
    "                                \n",
    "                            ax1.text(xlow+((xhigh-xlow)/20)\n",
    "                            , yhigh-((yhigh-ylow)/20), \n",
    "                            'Menses', color = '#AA0E24', fontsize = 14, fontstyle = 'normal', fontweight = 'bold', horizontalalignment='left')\n",
    "\n",
    "                                    \n",
    "                        ax1.axvline(value.loc[value['Menses']==1, 'Day'].iloc[i], linestyle = 'None',color='crimson', marker = 'o',  markersize = 14)\n",
    "\n",
    "                    ax1.set_xlim(0, 30)\n",
    "                    ax1.axvline(value.loc[value['Ovulation - countback']==1, 'Day'].iloc[0], linestyle = '-',color='blue', marker = 'o',  markersize = 15)\n",
    "                    if len(value.loc[value['Kassam']==1])>0:\n",
    "                        ax1.axvspan(value.loc[value['Kassam']==1, 'Day'].iloc[0]-1, value.loc[value['Kassam']==1, 'Day'].iloc[0]+1, color='violet', alpha=0.2)\n",
    "                        ax1.axvline(value.loc[value['Kassam']==1, 'Day'].iloc[0], linestyle = '--',color='violet', marker = 'None', alpha=0.5)\n",
    "                    #if len(value.loc[value['Kassam+4']==1])>0:\n",
    "                       # ax1.axvspan(value.loc[value['Kassam+4']==1, 'Day'].iloc[0], value.loc[value['Kassam+4']==1, 'Day'].max() , alpha=0.2, color='violet')\n",
    "                    if len(value.loc[value['Baird']==1])>0:\n",
    "                        ax1.axvspan(value.loc[value['Baird']==1, 'Day'].iloc[0]-1, value.loc[value['Baird']==1, 'Day'].iloc[0]+1, color='orange', alpha=0.2)\n",
    "                        ax1.axvline(value.loc[value['Baird']==1, 'Day'].iloc[0], linestyle = '--',color='orange', marker = 'None', alpha=0.5)\n",
    "                    #if len(value.loc[value['Bairdrange']==1])>0:\n",
    "                        #ax1.axvspan(value.loc[value['Bairdrange']==1, 'Day'].iloc[0], value.loc[value['Bairdrange']==1, 'Day'].max() , alpha=0.2, color='orange')\n",
    "\n",
    "                    #ax1.axvline(1, linestyle = '--',color='gray', marker = 'None')\n",
    "                    ax1.axvspan(1, 5, alpha=0.2, color='gray')\n",
    "                    #ax1.axvline(5, linestyle = '--',color='gray', marker = 'None')\n",
    "\n",
    "                    \n",
    "                    \n",
    "\n",
    "                            # Set limits\n",
    "\n",
    "                    #ax1.set_xlim(value.reset_index()['Day'].min() -2, value.reset_index()['Day'].max() + 2)\n",
    "\n",
    "                    # ax1.set_ylim(0, e_df['edata'].max() + 0.5)\n",
    "\n",
    "\n",
    "                    plt.legend('', frameon=False)\n",
    "                    ax1.legend('', frameon=False)\n",
    "\n",
    "                    ax2.set_ylabel('Oestradiol, pg/mL', color = '#DB2525', fontsize = 16)\n",
    "                    ax1.set_ylabel('Progesterone, pg/mL', color = 'limegreen', fontsize = 16)\n",
    "                    ax1.set_xlabel('Cycle day', fontsize = 16)\n",
    "                    plt.title('Player '+str(p) +', Cycle '+str(c)+ ', Frequency '+str(k), fontsize = 18)\n",
    "                    \n",
    "                    \n",
    "                    # plt.grid(True, axis = 'x' )\n",
    "                    #ax1.grid(True, which = 'both', axis = 'x', color='#DDDDDD', linestyle='--', linewidth=1 )\n",
    "                    plt.savefig('{}_{}_{}.png'.format(p, c, k), format='png', dpi=1200, bbox_inches='tight', facecolor='w', transparent=False)\n",
    "                    plt.show()'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import boto3\n",
    "import json\n",
    "import boto3.session\n",
    "# https://boto3.amazonaws.com/v1/documentation/api/latest/guide/dynamodb.html#querying-and-scanning\n",
    "from boto3.dynamodb.conditions import GreaterThan, Key, Attr\n",
    "import dynamo_pandas\n",
    "from dynamo_pandas import get_df, keys, put_df\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "import seaborn as sns\n",
    "from docx import Document\n",
    "from docx.shared import Mm\n",
    "from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
    "from docx.enum.table import WD_ALIGN_VERTICAL\n",
    "from docx.enum.text import WD_BREAK\n",
    "#from docx.enum.table import WD_ALIGN_HORIZONTAL\n",
    "from docx2pdf import convert\n",
    "\n",
    "def connect_dynamodb(region = 'eu-west-2', profile = 'default'):\n",
    "    # Connect to DynamoDB\n",
    "    my_session = boto3.session.Session(region_name = region, profile_name = profile)\n",
    "    global dynamodb\n",
    "    dynamodb = my_session.resource('dynamodb')\n",
    "\n",
    "connect_dynamodb('eu-west-2','default')\n",
    "\n",
    "#define tables\n",
    "table_athlete = dynamodb.Table('Athlete-qx7lirnxjfdzxoss6cmomxzgpe-staging')\n",
    "table_sample = dynamodb.Table('Sample-qx7lirnxjfdzxoss6cmomxzgpe-staging')\n",
    "table_measurement = dynamodb.Table('Measurement-qx7lirnxjfdzxoss6cmomxzgpe-staging')\n",
    "table_study = dynamodb.Table('Study-qx7lirnxjfdzxoss6cmomxzgpe-staging')\n",
    "table_testbatchcode = dynamodb.Table('TestBatchCode-qx7lirnxjfdzxoss6cmomxzgpe-staging')\n",
    "table_answer = dynamodb.Table('Answer-qx7lirnxjfdzxoss6cmomxzgpe-staging')\n",
    "table_organisation = dynamodb.Table('Organisation-qx7lirnxjfdzxoss6cmomxzgpe-staging')\n",
    "\n",
    "def get_org_from_dynamo(table):\n",
    "\n",
    "    org_df_meta=dynamo_pandas.get_df(table=table)\n",
    "\n",
    "    inTitles = False\n",
    "    while inTitles == False:\n",
    "        \n",
    "        org = input('Enter name of organisation: ')\n",
    "            # Is prog in response?\n",
    "    #         print(len(prog_df[prog_df.title == prog]))\n",
    "        if len(org_df_meta[org_df_meta.name == org]) == 1:\n",
    "            \n",
    "            inTitles = True\n",
    "            org_df = org_df_meta[org_df_meta.name == org] \n",
    "\n",
    "        else:\n",
    "            inTitles=False\n",
    "            print('Organisation not found. Please try again and ensure it is typed correctly')\n",
    "            \n",
    "                \n",
    "    print('Organisation Name: ' + str(org_df['name'].values[0]))      \n",
    "        \n",
    "    org_code = org_df['code'].values[0]\n",
    "    org_code_GCR = org_df['groupCanRead'].values[0]\n",
    "    print('Organisation code:', org_code)\n",
    "\n",
    "    return org_code_GCR\n",
    "\n",
    "#get programme from org\n",
    "\n",
    "def get_prog_from_org(org_code_GCR):  \n",
    "    \n",
    "    prog_df_meta=dynamo_pandas.get_df(table='Study-qx7lirnxjfdzxoss6cmomxzgpe-staging')\n",
    "\n",
    "    prog_df_org=prog_df_meta[prog_df_meta['groupCanRead']==org_code_GCR]\n",
    "\n",
    "    if len(prog_df_org)==0:\n",
    "        print('No programmes found for this organisation')\n",
    "    elif len(prog_df_org)==1:\n",
    "        prog=prog_df_org['title'].values[0]\n",
    "        print('This organisation has one programme:', prog)\n",
    "        prog_df=prog_df_org[prog_df_org['title']==prog]\n",
    "    else:\n",
    "        print('Please input programme from this list: ')\n",
    "        print(prog_df_org['title'])\n",
    "        \n",
    "        inTitles = False\n",
    "        while inTitles == False:\n",
    "            prog = input('Enter title of programme: ')\n",
    "            \n",
    "            if len(prog_df_org[prog_df_org.title == prog]) == 1:\n",
    "                inTitles = True\n",
    "                prog_df = prog_df_org[prog_df_org.title == prog] \n",
    "            else:\n",
    "                print('Programme name not found. Please try again and ensure it is typed correctly')\n",
    "\n",
    "    print('Programme title: ' + str(prog_df['title'].values[0]))      \n",
    "        \n",
    "    programme_id = prog_df['id'].values[0]\n",
    "    print('Programme ID:', programme_id)\n",
    "    return programme_id\n",
    "\n",
    "#Get participants\n",
    "def get_participants_from_study(programme_id, org_code_GCR):\n",
    "\n",
    "    par_df_meta=dynamo_pandas.get_df(table='Athlete-qx7lirnxjfdzxoss6cmomxzgpe-staging')\n",
    "    par_df_prog=par_df_meta[par_df_meta['athleteStudyId']==programme_id]\n",
    "    par_id_l=[]\n",
    "\n",
    "\n",
    "    if len(par_df_prog)==0:\n",
    "        print('No participants found for this programme')\n",
    "    elif len(par_df_prog)==1:\n",
    "        par=par_df_prog['name'].values[0]\n",
    "        par_df=par_df_prog[par_df_prog['name']==par]\n",
    "        par_id = par_df['id'].values[0]\n",
    "        par_id_l.append(par_id)\n",
    "        print('This study has one participant:', par)\n",
    "    else:\n",
    "        print('This programme has', len(par_df_prog), 'participants')\n",
    "        print('Please input participant name from this list | or type ALL to generate batch report: ')\n",
    "        print(par_df_prog['name'])\n",
    "        inTitles = False\n",
    "        while inTitles == False:\n",
    "            par = input('Enter name of participant, or enter ALL for batch report: ')\n",
    "            \n",
    "            if len(par_df_prog[par_df_prog.name == par]) == 1:\n",
    "                inTitles = True\n",
    "                par_df = par_df_prog[par_df_prog.name == par]\n",
    "                print('Participant Name: ' + str(par_df['name'].values[0]))  \n",
    "                par_id = par_df['id'].values[0]\n",
    "                par_id_l.append(par_id)\n",
    "                print('Participant ID:', par_id)\n",
    "                \n",
    "            elif (par=='ALL')|(par=='all')|(par=='All'):\n",
    "                par_df=par_df_prog\n",
    "                par_id_l=list(par_df['id'])\n",
    "                inTitles = True\n",
    "                print('Generating batch report for entire programme...')\n",
    "            else:\n",
    "                print('Participant not found. Please try again and ensure their name is typed correctly')\n",
    "\n",
    "    \n",
    "    return par_id_l\n",
    "\n",
    "def get_participants_from_org(org_code_GCR):\n",
    "\n",
    "    par_df_meta=dynamo_pandas.get_df(table='Athlete-qx7lirnxjfdzxoss6cmomxzgpe-staging')\n",
    "    par_id_l=[]\n",
    "    \n",
    "\n",
    "    org_code=org_code_GCR[:-4]\n",
    "    orgAdmin= org_code+'Admin'\n",
    "\n",
    "    par_df_org=par_df_meta.loc[par_df_meta['groupCanAdmin']==orgAdmin]\n",
    "    \n",
    "    #print(len(par_df_org_id_list), len(par_df_prog_id_list))\n",
    "\n",
    "    if len(par_df_org)==0:\n",
    "        print('No participants found for this programme')\n",
    "    elif len(par_df_org)==1:\n",
    "        par=par_df_org['name'].values[0]\n",
    "        par_df=par_df_org[par_df_org['name']==par]\n",
    "        par_id= par_df['id'].values[0]\n",
    "        par_id_l.append(par_id)\n",
    "        print('This organisation has one participant:', par)\n",
    "    else:\n",
    "        print('This organisation has', len(par_df_org), 'participants')\n",
    "        print('Please input participant name from this list | or type ALL to generate batch report: ')\n",
    "        print(par_df_org['name'])\n",
    "        inTitles = False\n",
    "        while inTitles == False:\n",
    "            par = input('Enter name of participant, or enter ALL for batch report: ')\n",
    "            \n",
    "            if len(par_df_org[par_df_org.name == par]) == 1:\n",
    "                inTitles = True\n",
    "                par_df = par_df_org[par_df_org.name == par]\n",
    "                print('Participant Name: ' + str(par_df['name'].values[0]))  \n",
    "                \n",
    "                par_id = par_df['id'].values[0]\n",
    "                par_id_l.append(par_id)\n",
    "                print('Participant ID:', par_id)\n",
    "                \n",
    "            elif (par=='ALL')|(par=='all')|(par=='All'):\n",
    "                par_df=par_df_org\n",
    "                par_id_l=list(par_df['id'])\n",
    "                inTitles = True\n",
    "                print('Generating batch report for entire organisation...')\n",
    "            else:\n",
    "                print('Participant not found. Please try again and ensure their name is typed correctly')\n",
    "\n",
    "    \n",
    "    return par_id_l\n",
    "\n",
    "\n",
    "def get_test_batch_codes(table1):\n",
    "    #Fetch test batch code ids for estradiol and progesterone\n",
    "    tbc_df_meta=dynamo_pandas.get_df(table=table1)\n",
    "    #tbc_df_meta_p = tbc_df_meta[tbc_df_meta['participantId']==par]\n",
    "    \n",
    "    estradiol_tbc_l=list(tbc_df_meta.loc[(tbc_df_meta['name']=='PTP_ELISA_IBL_ESTRADIOL') | (tbc_df_meta['name']=='ELISA_IBL_ESTRADIOL'), 'id'])\n",
    "    progesterone_tbc_l=list(tbc_df_meta.loc[(tbc_df_meta['name']=='PTP_ELISA_IBL_PROGESTERONE') | (tbc_df_meta['name']=='ELISA_IBL_PROGESTERONE'), 'id'])\n",
    "\n",
    "    #estradiol_tbc_id = tbc_df_meta.loc[tbc_df_meta['name']=='ELISA_IBL_ESTRADIOL', 'id'].iloc[0] \n",
    "    #progesterone_tbc_id = tbc_df_meta.loc[tbc_df_meta['name']=='ELISA_IBL_PROGESTERONE', 'id'].iloc[0] \n",
    "\n",
    "    return estradiol_tbc_l, progesterone_tbc_l\n",
    "\n",
    "\n",
    "\n",
    "def scan_table(dynamoTable, filterExp, expAttrNames):\n",
    "    \n",
    "    \n",
    "    results = []\n",
    "    projExp = ''\n",
    "    fExp = ''\n",
    "    \n",
    "    # Construct ProjectionExpression string\n",
    "    for key, value in expAttrNames.items() :\n",
    "        if projExp == '':\n",
    "            projExp = key\n",
    "        else:\n",
    "            projExp = projExp + ', ' + key \n",
    "        \n",
    "    # Construct FilterExpression string\n",
    "    for key, value in filterExp.items() :\n",
    "        if fExp == '':\n",
    "            fExp = 'Attr(\\'{}\\').eq(\\'{}\\')'.format(key,value)\n",
    "        else:\n",
    "            fExp = fExp + ' & Attr(\\'{}\\').eq(\\'{}\\')'.format(key,value) \n",
    "    \n",
    "    \n",
    "    response = dynamoTable.scan(\n",
    "        ProjectionExpression=projExp,\n",
    "        FilterExpression = eval(fExp),\n",
    "        ExpressionAttributeNames = expAttrNames # Need to use ExpressionAttributeNames as some field names are reserved keywords (e.g. value)\n",
    "    )\n",
    "    results = response['Items']\n",
    "    \n",
    "    # Get all items (https://dynobase.dev/dynamodb-python-with-boto3/#scan)\n",
    "    while 'LastEvaluatedKey' in response:\n",
    "        response = dynamoTable.scan(ExclusiveStartKey = response['LastEvaluatedKey'],\n",
    "                                    ProjectionExpression=projExp,\n",
    "                                    FilterExpression = eval(fExp),\n",
    "                                    ExpressionAttributeNames = expAttrNames # Need to use ExpressionAttributeNames as value is a reserved keyword\n",
    "                                          )\n",
    "        results.extend(response['Items'])\n",
    "\n",
    "    return results\n",
    "\n",
    "def getmostrecentkit(par):\n",
    "    kit_measurements = scan_table(table_measurement,\n",
    "                                    filterExp = {'measurementAthleteId' : par, \n",
    "                                    'measurementTestBatchCodeId' : estradiol_tbc_id},\n",
    "                                    expAttrNames = {'#i' : 'id', '#b' : 'barcode',\n",
    "                                                    '#m' : 'measurementAthleteId' , \n",
    "                                                    '#v' : 'value', \n",
    "                                                    '#k' : 'kit', \n",
    "                                                    '#c' : 'collectedAt', \n",
    "                                                    '#t':'measurementTestBatchCodeId'})\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    measurement_df = pd.DataFrame.from_dict(kit_measurements)\n",
    "    if len(measurement_df)==0:\n",
    "        print('No hormone measurements found for this participant')\n",
    "    else:\n",
    "    \n",
    "        mostrecentkit=[]\n",
    "\n",
    "\n",
    "        kitlist=measurement_df.loc[measurement_df['measurementAthleteId']==par, 'kit'].unique()\n",
    "        print('Finding most recent kit...')\n",
    "        if len(kitlist)>0:\n",
    "            kitnums = [kit[3:] for kit in kitlist]\n",
    "            maxkit=max(kitnums)\n",
    "            df_par_kit=measurement_df[(measurement_df['measurementAthleteId']==par) & (measurement_df['kit']=='KIT'+maxkit)]\n",
    "            \n",
    "            #print(len(df_par_kit))\n",
    "            mostrecentkit.append(df_par_kit)\n",
    "        if len(mostrecentkit)==0:\n",
    "            print('No kits found for this participant ID:', par)\n",
    "        else:\n",
    "\n",
    "            df_mostrecentkit=pd.concat(mostrecentkit)\n",
    "\n",
    "            return df_mostrecentkit\n",
    "\n",
    "def getallkitnums(par):\n",
    "    kit_measurements = scan_table(table_measurement,\n",
    "                                    filterExp = {'measurementAthleteId' : par},\n",
    "                                    expAttrNames = {'#i' : 'id', '#b' : 'barcode',\n",
    "                                                    '#m' : 'measurementAthleteId' , \n",
    "                                                    '#v' : 'value', \n",
    "                                                    '#k' : 'kit', \n",
    "                                                    '#c' : 'collectedAt', \n",
    "                                                    '#t':'measurementTestBatchCodeId'})\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    measurement_df = pd.DataFrame.from_dict(kit_measurements)\n",
    "    if len(measurement_df)==0:\n",
    "        print('No hormone measurements found for this participant')\n",
    "        kitlist=[]\n",
    "        return kitlist\n",
    "    else:\n",
    "        kitlist=measurement_df.loc[measurement_df['measurementAthleteId']==par, 'kit'].unique()\n",
    "        kitnums = [kit[3:] for kit in kitlist]\n",
    "        print('Found kits:', kitnums)\n",
    "    \n",
    "        \n",
    "        return kitlist\n",
    "\n",
    "#---------------------------------------------------------\n",
    "\n",
    "def get_e_df_and_p_df(kitnum, par, estradiol_tbc_id_l, progesterone_tbc_id_l):\n",
    "\n",
    "    estradiol_measurements = scan_table(table_measurement,\n",
    "                                    filterExp = {'measurementAthleteId' : par,\n",
    "                                                'kit' : kitnum},\n",
    "                                    expAttrNames = {'#i' : 'id', \n",
    "                                                    '#b' : 'barcode' , \n",
    "                                                    '#v' : 'value', \n",
    "                                                    '#k' : 'kit', \n",
    "                                                    '#c' : 'collectedAt',\n",
    "                                                    '#t':'measurementTestBatchCodeId'})\n",
    "\n",
    "    \n",
    "        \n",
    "# print(estradiol_measurements)\n",
    "    estradiol_df = pd.DataFrame.from_dict(estradiol_measurements)\n",
    "\n",
    "    estradiol_df = estradiol_df.loc[estradiol_df['measurementTestBatchCodeId'].isin(estradiol_tbc_id_l)]\n",
    "    #print(estradiol_tbc_id)\n",
    "    #estradiol_df = estradiol_df.loc[estradiol_df['measurementTestBatchCodeId']==estradiol_tbc_id]\n",
    "\n",
    "    print('Number of estradiol measurements retrieved: {}'.format(len(estradiol_df)))\n",
    "    \n",
    "    if len(estradiol_df)>0:\n",
    "        estradiol_df = estradiol_df.rename(columns = {\"value\" : \"edata\"})\n",
    "        estradiol_df['collectedAt'] = pd.to_datetime(estradiol_df['collectedAt'])\n",
    "        estradiol_df['collectedDate'] = pd.to_datetime(estradiol_df['collectedAt']).dt.date    \n",
    "        estradiol_df = estradiol_df.sort_values(by=['collectedAt'], ascending = True, ignore_index = True).drop(columns = ['collectedAt'])\n",
    "        #estradiol_df.loc[estradiol_df['edata'].str.contains('<'), 'edata'] = estradiol_df['edata'].str.replace('<','')\n",
    "        #estradiol_df.loc[estradiol_df['edata'].str.contains('>'), 'edata'] = estradiol_df['edata'].str.replace('>','')\n",
    "        estradiol_df['edata_r'] = estradiol_df['edata']\n",
    "        estradiol_df.loc[estradiol_df['edata'].str.contains('<', na=False), 'edata'] = np.nan\n",
    "        estradiol_df.loc[estradiol_df['edata'].str.contains('>', na=False), 'edata'] = np.nan\n",
    "\n",
    "            \n",
    "        estradiol_df['edata'] = estradiol_df['edata'].astype(float)\n",
    "        #estradiol_df['edata_r'] = estradiol_df['edata_r'].astype(float)\n",
    "        \n",
    "        estradiol_df['linear'] = estradiol_df['edata'].interpolate(method='linear')\n",
    "        e_df = estradiol_df\n",
    "        e_df['collectedDate'] = pd.to_datetime(e_df['collectedDate'])\n",
    "        e_df['collectedDate'] = pd.to_datetime(e_df['collectedDate'].dt.strftime('%d/%m/%Y')) \n",
    "        print(e_df['collectedDate'].min(), e_df['collectedDate'].max())\n",
    "        e_df = e_df.set_index('collectedDate')\n",
    "        e_df= e_df.loc[~e_df.index.duplicated(), :]\n",
    "        e_df = e_df.resample('1D').asfreq()\n",
    "        e_df['linear'] = e_df['edata'].interpolate(method='linear')\n",
    "        e_df = e_df.drop(columns = ['barcode','id','kit'])\n",
    "\n",
    "        \n",
    "        # Calculate rolling average with a window\n",
    "        window = 3\n",
    "        e_df['rolling'] = e_df['linear'].rolling(window).sum()/window\n",
    "        e_df['rolling'] = e_df['rolling'].shift(periods = -1)\n",
    "        #e_df.loc[e_df['edata_r'].str.contains('<', na=False), 'rolling'] = np.nan\n",
    "        #e_df.loc[e_df['edata_r'].str.contains('>', na=False), 'rolling'] = np.nan\n",
    "        e_df_n=e_df['edata'].dropna()\n",
    "\n",
    "        n_max=e_df_n.index.max()\n",
    "\n",
    "        e_df.loc[e_df.index>n_max, 'rolling'] = np.nan\n",
    "\n",
    "        e_df.head()\n",
    "\n",
    "    progesterone_measurements = scan_table(table_measurement,\n",
    "                                    filterExp = {'measurementAthleteId' : par,\n",
    "                                                'kit' : kitnum},\n",
    "                                    expAttrNames = {'#i' : 'id', \n",
    "                                                    '#b' : 'barcode' , \n",
    "                                                    '#v' : 'value', \n",
    "                                                    '#k' : 'kit', \n",
    "                                                    '#c' : 'collectedAt',\n",
    "                                                    '#t':'measurementTestBatchCodeId'})\n",
    "\n",
    "    \n",
    "            \n",
    "    # print(estradiol_measurements)\n",
    "    progesterone_df = pd.DataFrame.from_dict(progesterone_measurements)\n",
    "    progesterone_df = progesterone_df.loc[progesterone_df['measurementTestBatchCodeId'].isin(progesterone_tbc_id_l)]\n",
    "    \n",
    "    #progesterone_df=progesterone_df.loc[progesterone_df['measurementTestBatchCodeId']==progesterone_tbc_id]\n",
    "\n",
    "    \n",
    "    print('Number of progesterone measurements retrieved: {}'.format(len(progesterone_df)))\n",
    "    if len(progesterone_df)>0:\n",
    "        progesterone_df = progesterone_df.rename(columns = {\"value\" : \"pdata\"})\n",
    "        progesterone_df['collectedAt'] = pd.to_datetime(progesterone_df['collectedAt'])\n",
    "        progesterone_df['collectedDate'] = pd.to_datetime(progesterone_df['collectedAt']).dt.date    \n",
    "        progesterone_df = progesterone_df.sort_values(by=['collectedAt'], ascending = True, ignore_index = True).drop(columns = ['collectedAt'])\n",
    "        #progesterone_df.loc[progesterone_df['pdata'].str.contains('<'), 'pdata'] = progesterone_df['pdata'].str.replace('<','')\n",
    "        #progesterone_df.loc[progesterone_df['pdata'].str.contains('>'), 'pdata'] = progesterone_df['pdata'].str.replace('>','')\n",
    "        progesterone_df['pdata_r'] = progesterone_df['pdata']\n",
    "        progesterone_df.loc[progesterone_df['pdata'].str.contains('<', na=False), 'pdata'] = np.nan\n",
    "        progesterone_df.loc[progesterone_df['pdata'].str.contains('>', na = False), 'pdata'] = np.nan\n",
    "        \n",
    "        progesterone_df['pdata'] = progesterone_df['pdata'].astype(float)\n",
    "        #progesterone_df['pdata_r'] = progesterone_df['pdata_r'].astype(float)\n",
    "        \n",
    "        progesterone_df['linear'] = progesterone_df['pdata'].interpolate(method='linear')\n",
    "        p_df = progesterone_df\n",
    "        p_df['collectedDate'] = pd.to_datetime(p_df['collectedDate'])\n",
    "        p_df = p_df.set_index('collectedDate')\n",
    "        p_df= p_df.loc[~p_df.index.duplicated(), :]\n",
    "        p_df = p_df.resample('1D').asfreq()\n",
    "        p_df['linear'] = p_df['pdata'].interpolate(method='linear')\n",
    "        p_df = p_df.drop(columns = ['barcode','id','kit'])\n",
    "        window = 3\n",
    "        p_df['rolling'] = p_df['linear'].rolling(window).sum()/window\n",
    "        p_df['rolling'] = p_df['rolling'].shift(periods = -1)\n",
    "        p_df.loc[p_df['pdata_r'].str.contains('<', na=False), 'rolling'] = np.nan\n",
    "        p_df.loc[p_df['pdata_r'].str.contains('>', na=False), 'rolling'] = np.nan\n",
    "        p_df.head()\n",
    "    \n",
    "        return e_df, p_df \n",
    "    #-----------------------------------------------------------------------------------------------------------------------\n",
    "def get_samples(par, kitnum):\n",
    "    samples = scan_table(dynamoTable = table_sample,\n",
    "                    #  filterExp = {'sampleAthleteId' : athlete_id},\n",
    "                    filterExp = {'sampleAthleteId' : par,\n",
    "                                'kit' : kitnum},\n",
    "                    expAttrNames = {'#b' : 'barcode', '#c' : 'collectedAt'})\n",
    "\n",
    "    print('Number of samples retrieved: {}'.format(len(samples)))\n",
    "    # print(samples)\n",
    "    samples_df = pd.DataFrame.from_dict(samples)\n",
    "    samples_df['collectedAt'] = pd.to_datetime(samples_df['collectedAt'])\n",
    "    samples_df['collectedDate'] = pd.to_datetime(samples_df['collectedAt']).dt.date\n",
    "    samples_df['collectedTime'] = pd.to_datetime(samples_df['collectedAt']).dt.time\n",
    "    samples_df = samples_df.sort_values(by=['collectedAt'], ascending = True, ignore_index = True).drop(columns = ['collectedAt'])\n",
    "    return samples_df\n",
    "def get_answers(par, kitnum):\n",
    "    answers = scan_table(dynamoTable = table_answer,\n",
    "                            filterExp = {'athleteID' : par, \n",
    "                                                'kit' : kitnum},\n",
    "                            expAttrNames = {'#v' : 'value', '#c' : 'collectedAt'})\n",
    "        \n",
    "    answers_df = pd.DataFrame.from_dict(answers)\n",
    "    if len(answers_df)==0:\n",
    "        print('No answers data found')\n",
    "    #get_pdf_no_symptoms(samples_df, e_df, p_df, par, export_path)\n",
    "    else:\n",
    "        #answers = dynamo_pandas.get_df(table='Answer-qx7lirnxjfdzxoss6cmomxzgpe-staging') \n",
    "        #answers_df=answers.loc[(answers['athleteID']==par) & (answers['kit']==kitnum)]\n",
    "        #answers_df=answers_df.filter(['athleteID','value', 'kit', 'collectedAt'], axis=1)\n",
    "        #answers_df = answers_df.reset_index()\n",
    "        answers_df['collectedDate'] = pd.to_datetime(answers_df['collectedAt']).dt.date   \n",
    "\n",
    "    # Initialise Bleeding column\n",
    "        answers_df['Bleeding'] = 'No'\n",
    "\n",
    "        # Sort answers by collectedAt date\n",
    "        answers_df = answers_df.sort_values(by=['collectedAt'], ascending = True, ignore_index = True)\n",
    "\n",
    "        answers_df['collectedAt'] = pd.to_datetime(answers_df['collectedAt'])\n",
    "        answers_df['collectedDate'] = pd.to_datetime(answers_df['collectedAt']).dt.date\n",
    "        answers_df['collectedDate'] = pd.to_datetime(answers_df['collectedDate'])\n",
    "\n",
    "        # Remove all answers after the last date of the samples_df\n",
    "        # answers_df = answers_df.astype({'collectedDate' : 'datetime64[ns]'}) # Change type\n",
    "        # print(answers_df['collectedDate'].dtypes)\n",
    "        # print(samples_df['collectedDate'].dtypes)\n",
    "        # print(answers_df)\n",
    "        # print(samples_df['collectedDate'].iloc[-1])\n",
    "        # answers_df = answers_df[answers_df['collectedDate'] >= samples_df['collectedDate'].iloc[0]]\n",
    "        # answers_df = answers_df[answers_df['collectedDate'] <= samples_df['collectedDate'].iloc[-1]]\n",
    "        answers_df = answers_df.reset_index(drop = True)\n",
    "        #----\n",
    "\n",
    "        answers_df['Your Sleep'] = np.nan\n",
    "\n",
    "        symptoms = ['Backache',\n",
    "                    'Joint pain',\n",
    "                    'Abdominal pain',\n",
    "                    'Abdominal cramps',\n",
    "                    'Breast tenderness',\n",
    "                    'Headaches',\n",
    "                    'Heavy legs',\n",
    "                    'Muscle spasms',\n",
    "                    'Clumsiness',\n",
    "                    'Gastrointestinal upset',\n",
    "                    'Food craving',\n",
    "                    'Binge eating',\n",
    "                    'Reduced appetite',\n",
    "                    'Fatigue',\n",
    "                    'Increased sex drive',\n",
    "                    'Skin problems',\n",
    "                    'Common cold symptoms',\n",
    "                    'Low mood',\n",
    "                    'Sadness',\n",
    "                    'Tearful',\n",
    "                    'Mood swings',\n",
    "                    'Anxiety',\n",
    "                    'Paranoia',\n",
    "                    'Concentration loss',\n",
    "                    'Confusion']\n",
    "\n",
    "        # for symptom in symptoms:\n",
    "        #     answers_df.insert(len(answers_df.columns),\n",
    "        #                      symptom,\n",
    "        #                      allow_duplicates=False)\n",
    "\n",
    "        symptoms_df = pd.DataFrame(0, index=np.arange(len(answers_df)), columns=symptoms)\n",
    "        # print(symptoms_df)\n",
    "        answers_df = answers_df.join(symptoms_df)\n",
    "        answers_df.head()\n",
    "\n",
    "        \n",
    "        # Find all days where Bleeding is reported\n",
    "        for i, row in answers_df.iterrows():\n",
    "            drow = json.loads(row['value'])         \n",
    "            for key in drow.keys():\n",
    "        #         print(drow[key])\n",
    "                if drow[key]['questionTitle'] == 'Bleeding':\n",
    "                    if drow[key]['value'] == 'Yes':\n",
    "                        answers_df.loc[i,'Bleeding'] = 'Yes' # Sets zero when PAR reports bleeding. This is for charting later.\n",
    "                    \n",
    "                elif drow[key]['questionTitle'] == 'Your Sleep':\n",
    "                    answers_df.loc[i, 'Your Sleep'] = drow[key]['value']\n",
    "                    \n",
    "                elif drow[key]['questionTitle'] == 'Rate your Symptoms':\n",
    "        #             print(drow[key]['value'])\n",
    "                    for item in drow[key]['value']: # Each item in list is a dict with label and value\n",
    "                        if 'value' in item:\n",
    "                            answers_df.loc[i, item['label']] = item['value']\n",
    "\n",
    "        bleedingdays = answers_df[answers_df['Bleeding'] == 'Yes']['collectedDate']\n",
    "        bleedingdays\n",
    "\n",
    "    return answers_df  \n",
    "#---------------------------------------------------------------------------------------------------------------------\n",
    "def get_figure(answers_df, e_df, p_df):\n",
    "        \n",
    "    \n",
    "    from datetime import timedelta\n",
    "    if len(answers_df)==0:\n",
    "        print('No bleeding recorded for this participant')\n",
    "    else:\n",
    "        bleedingdays = answers_df.loc[answers_df['Bleeding'] == 'Yes','collectedDate']\n",
    "    #bleedingdays.reset_index()\n",
    "    fig, (ax1) = plt.subplots(nrows=1, ncols=1, sharex = True, figsize=(10,5))\n",
    "    ax2 = ax1.twinx()\n",
    "    e_max=e_df.edata.max()\n",
    "    \n",
    "    e_df.loc[e_df['rolling']>e_max, 'rolling']==np.nan\n",
    "\n",
    "\n",
    "    # Plot objects\n",
    "\n",
    "    # e_df.reset_index().plot.area(ax = ax1, x='collectedDate', y='rolling', \n",
    "    #          color = '#FFC8D344', linewidth = 5)\n",
    "    # e_df.reset_index().plot(ax = ax1, x='collectedDate', y='rolling', \n",
    "    #          color = '#DB2525')\n",
    "    # p_df.reset_index().plot.area(ax = ax2, x='collectedDate', y='rolling', \n",
    "    #          color = '#BAF67F44', linewidth = 5)\n",
    "    \n",
    "    e_df.reset_index().plot(ax = ax1, x='collectedDate', y='rolling', \n",
    "            color = '#FFC8D3', linewidth=6, label='Oestradiol Average')\n",
    "    e_df.reset_index().plot(ax = ax1, x='collectedDate', y='edata', \n",
    "            color = '#DB2525', marker = 'o', markersize = 6, linestyle = 'none', label = 'Oestradiol')\n",
    "    p_df.reset_index().plot(ax = ax2, x='collectedDate', y='rolling', \n",
    "            color = 'palegreen', linewidth=6, label='Progesterone Average')\n",
    "    p_df.reset_index().plot(ax = ax2, x='collectedDate', y='pdata', \n",
    "            color = 'limegreen', marker = 'o', markersize = 6, linestyle = 'none', label = 'Progesterone')\n",
    "\n",
    "    #answers_df.reset_index().plot(ax = ax1, x='collectedDate', y = 'Bleeding',\n",
    "                    #color = '#DB2525', marker = 'o', markersize = 10, linestyle = 'none')\n",
    "\n",
    "    ylow, yhigh = ax1.get_ylim()\n",
    "    xlow, xhigh = ax1.get_xlim()\n",
    "    if len(answers_df)>0:\n",
    "        if len(bleedingdays)>0:\n",
    "            for i in range(len(bleedingdays)):\n",
    "\n",
    "\n",
    "                if i == 0:\n",
    "                    \n",
    "                    ax1.text(xlow+((xhigh-xlow)/20)\n",
    "                    , yhigh-((yhigh-ylow)/20), \n",
    "                    'Menses', \n",
    "                            color = '#AA0E24', fontsize = 14, fontstyle = 'normal', fontweight = 'bold', horizontalalignment='left')\n",
    "                        \n",
    "                ax1.axvline(bleedingdays.iloc[i], linestyle = 'None',color='crimson', marker = 'o',  markersize = 14)\n",
    "        else:\n",
    "            print('No bleeding recorded for this participant')\n",
    "\n",
    "        # Set limits\n",
    "\n",
    "    ax1.set_xlim(p_df.reset_index()['collectedDate'].min() + timedelta(days = -2), \n",
    "                p_df.reset_index()['collectedDate'].max() + timedelta(days = 2))\n",
    "\n",
    "    # ax1.set_ylim(0, e_df['edata'].max() + 0.5)\n",
    "\n",
    "    # Time axis formatting\n",
    "    plt.gcf().autofmt_xdate()\n",
    "    #set ticks every week\n",
    "    # ax1.xaxis.set_major_locator(mdates.WeekdayLocator())\n",
    "    #set major ticks format\n",
    "    ax1.xaxis.set_major_formatter(mdates.DateFormatter('%b %d'))\n",
    "\n",
    "    ax1.set_title('Oestradiol and Progesterone: {}, {}'.format(par, kitnum), fontsize = 18)\n",
    "    ax1.set_ylabel('Oestradiol, pg/mL', color = '#DB2525', fontsize = 16)\n",
    "    ax2.set_ylabel('Progesterone, pg/mL', color = 'limegreen', fontsize = 16)\n",
    "    ax1.set_xlabel('Date of sample collection', fontsize = 16)\n",
    "    ax1.get_legend().remove()\n",
    "    ax2.get_legend().remove()\n",
    "    # plt.grid(True, axis = 'x' )\n",
    "    ax1.grid(True, which = 'both', axis = 'x', color='#DDDDDD', linestyle='--', linewidth=1 )\n",
    "    plt.savefig('{}_{}_EPchart.png'.format(par, kitnum), format='png', dpi=1200, bbox_inches='tight', facecolor='w', edgecolor='w', transparent=False)\n",
    "    plt.show()\n",
    "\n",
    "    return ax1\n",
    "\n",
    "listofdfs = []\n",
    "connect_dynamodb('eu-west-2','default')\n",
    "org_df_meta=dynamo_pandas.get_df(table='Organisation-qx7lirnxjfdzxoss6cmomxzgpe-staging')\n",
    "print('Please input organisation name from this list: ')    \n",
    "print(org_df_meta.name)\n",
    "org_code_gcr=get_org_from_dynamo('Organisation-qx7lirnxjfdzxoss6cmomxzgpe-staging')\n",
    "org_code=org_code_gcr[:-4]\n",
    "print('...')\n",
    "inTitles = False\n",
    "while inTitles == False:\n",
    "    print('Do you want to choose a programme from this organisation? (y/n)')\n",
    "    prog_yn = input('Do you want to choose a programme? (y/n): ')\n",
    "    if (prog_yn == 'y') | (prog_yn == 'Y'):\n",
    "        inTitles=True\n",
    "        prog_id= get_prog_from_org(org_code_gcr)\n",
    "        print('...')\n",
    "        par_id= get_participants_from_study(prog_id, org_code_gcr)\n",
    "        #batch_yn = input('Do you want to generate a batch report for all particpants in this programme? (y/n): ')\n",
    "    elif (prog_yn == 'n') | (prog_yn == 'N'):\n",
    "        par_id =get_participants_from_org(org_code_gcr)\n",
    "        inTitles =True\n",
    "\n",
    "for par in par_id:\n",
    "    estradiol_tbc_id,progesterone_tbc_id = get_test_batch_codes('TestBatchCode-qx7lirnxjfdzxoss6cmomxzgpe-staging')\n",
    "   \n",
    "    #mostrecentkitdf= getmostrecentkit(par)\n",
    "    #kitnum=mostrecentkitdf.loc[mostrecentkitdf['measurementAthleteId']==par, 'kit'].iloc[0]\n",
    "    #print('Most recent kit is:', kitnum)\n",
    "    kitlist=getallkitnums(par)\n",
    "    if len(kitlist)>0:\n",
    "        \n",
    "        for kitnum in kitlist:\n",
    "        \n",
    "            samples_df=get_samples(par, kitnum)\n",
    "           # print(samples_df)\n",
    "            #export_path=make_dirs(org_code_gcr, kitnum, par, directory)\n",
    "            e_df, p_df = get_e_df_and_p_df(kitnum, par, estradiol_tbc_id,progesterone_tbc_id)\n",
    "            print(e_df.index,\n",
    "                   p_df.index)\n",
    "            answers_df=get_answers(par, kitnum)\n",
    "            ep_chart=get_figure(answers_df, e_df, p_df)\n",
    "\n",
    "            #DROP ROLLING AND LINEAR COLUMNS\n",
    "\n",
    "            e_df = e_df.drop(columns=['rolling', 'linear'])\n",
    "            p_df = p_df.drop(columns=['rolling', 'linear'])\n",
    "            \n",
    "            #e_df.to_csv('{}_{}_e_df.csv'.format(par_id, kitnum))\n",
    "            #p_df.to_csv('{}_{}_p_df.csv'.format(par_id, kitnum))\n",
    "\n",
    "            #MERGE E_DF AND P_DF ON DATE\n",
    "            df = pd.merge(e_df, p_df, on='collectedDate')\n",
    "            df['kit'] = kitnum\n",
    "            df['participantId'] = par\n",
    "            # if answers df bleeding exists\n",
    "            df['Bleeding'] = 0\n",
    "            if 'Bleeding' in answers_df.columns:\n",
    "                bleedingdays = answers_df[answers_df['Bleeding'] == 'Yes']['collectedDate']\n",
    "                #match bleedingdays to df collectedDate ==1 \n",
    "\n",
    "                \n",
    "                \n",
    "                df = df.reset_index()\n",
    "                df.loc[df['collectedDate'].isin(bleedingdays), 'Bleeding'] = 1\n",
    "                \n",
    "\n",
    "            df = df.sort_values(by='collectedDate')\n",
    "            listofdfs.append(df)\n",
    "\n",
    "fulldf=pd.concat(listofdfs)\n",
    "\n",
    "fulldf.to_csv('fulldf_{}.csv'.format(org_code))\n",
    "print(fulldf)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('fulldf_MNC.csv', parse_dates=['collectedDate'])\n",
    "print(data)\n",
    "# Sort data by participantId and collectedDate\n",
    "data.sort_values(by=['participantId', 'collectedDate'], inplace=True)\n",
    "\n",
    "# Initialize columns\n",
    "data['Cycle'] = -1\n",
    "data['Countback_Day'] = False\n",
    "data['Ovulation_Detected'] = 0\n",
    "data['Ovulation_Day'] = 0\n",
    "data['P_Data_Difference'] = None\n",
    "\n",
    "# Function to process data for each participant\n",
    "def process_data(group):\n",
    "    \n",
    "    if len(group)>=5:\n",
    "        #print(len(group))\n",
    "        min_cycle_length = 21\n",
    "        default_cycle_length = 28\n",
    "        countback_days_prior = 14\n",
    "        ovulation_difference_threshold = 1.5\n",
    "        ovulation_day_shift = 4\n",
    "\n",
    "        cycle = 0\n",
    "        last_bleeding_date = None\n",
    "        for i, row in group.iterrows():\n",
    "            if row['Bleeding'] == 1:\n",
    "                if last_bleeding_date is None:\n",
    "                    last_bleeding_date = row['collectedDate']\n",
    "                elif (row['collectedDate'] - last_bleeding_date).days >= min_cycle_length:\n",
    "                    cycle += 1\n",
    "                    last_bleeding_date = row['collectedDate']\n",
    "            data.loc[i, 'Cycle'] = cycle\n",
    "\n",
    "            #print(len(group))\n",
    "\n",
    "        if cycle == 0:\n",
    "            # No bleeding days found, assume cycle length is 28 days\n",
    "            start_date = group['collectedDate'].min()\n",
    "            group['Cycle'] = ((group['collectedDate'] - start_date).dt.days // default_cycle_length).values\n",
    "\n",
    "        '''for i, row in group.iterrows():\n",
    "            if (row['Bleeding'] == 1):\n",
    "                countback_date = row['collectedDate'] - pd.Timedelta(days=countback_days_prior)\n",
    "                data.loc[data['collectedDate'] == countback_date, 'Countback_Day'] = 1\n",
    "                continue\n",
    "\n",
    "            elif \n",
    "                countback_date \n",
    "                data.loc[i, 'Countback_Day'] = 0\n",
    "\n",
    "        #\n",
    "\n",
    "                \n",
    "        print(len(group), '...', countback_date, '...', cycle, group['participantId'].iloc[0])\n",
    "        countback_minus_4 = group.loc[group['Countback_Day']==1].index - 4\n",
    "        baseline_p_data = group.loc[:countback_minus_4[0], 'pdata'].mean()\n",
    "\n",
    "        group['P_Data_Difference'] = (group['pdata'] - baseline_p_data) / baseline_p_data\n",
    "        max_difference_index = group['P_Data_Difference'].idxmax()\n",
    "\n",
    "        if group.loc[max_difference_index, 'P_Data_Difference'] > ovulation_difference_threshold:\n",
    "            data.loc[max_difference_index, 'Ovulation_Detected'] = 1\n",
    "            ovulation_day_index = max_difference_index - ovulation_day_shift\n",
    "            data.loc[ovulation_day_index, 'Ovulation_Day'] = 1\n",
    "        #print(len(group))'''\n",
    "        return group\n",
    "\n",
    "# Group data by participantId and process it\n",
    "processed_data = data.groupby('participantId').apply(process_data)\n",
    "\n",
    "\n",
    "\n",
    "# Save the result to a new CSV file\n",
    "processed_data.to_csv('processed_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnc = pd.read_csv('fulldf_MNC.csv')\n",
    "vnz = pd.read_csv('fulldf_VNZ.csv')\n",
    "JUV = pd.read_csv('fulldf_JUV.csv')\n",
    "\n",
    "merged = pd.concat([mnc, vnz, JUV])\n",
    "\n",
    "merged.to_csv('merged.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##interpolate\n",
    "\n",
    "\n",
    "\n",
    "every3rdfull = every3rdfull.interpolate(method='linear', limit_direction='forward', axis=0)\n",
    "everyotherfull = everyotherfull.interpolate(method='linear', limit_direction='forward', axis=0)\n",
    "twiceaweekfull = twiceaweekfull.interpolate(method='linear', limit_direction='forward', axis=0)\n",
    "oneperweekfull = oneperweekfull.interpolate(method='linear', limit_direction='forward', axis=0)\n",
    "\n",
    "every3rdfull.to_csv('C:/Users/KatherineRidley/OneDrive - Mint Diagnostics Ltd/Documents/Data Analysis/every3rdfullinterpolated.csv')\n",
    "everyotherfull.to_csv('C:/Users/KatherineRidley/OneDrive - Mint Diagnostics Ltd/Documents/Data Analysis/everyotherfullinterpolated.csv')\n",
    "twiceaweekfull.to_csv('C:/Users/KatherineRidley/OneDrive - Mint Diagnostics Ltd/Documents/Data Analysis/twiceaweekfullinterpolated.csv')\n",
    "oneperweekfull.to_csv('C:/Users/KatherineRidley/OneDrive - Mint Diagnostics Ltd/Documents/Data Analysis/oneperweekfullinterpolated.csv')\n",
    "\n",
    "fulldata = data\n",
    "\n",
    "print(every3rdfull.columns)\n",
    "\n",
    "##baseline for interpolated data\n",
    "\n",
    "def baseline(df, player, cycle, column, base):\n",
    "    indata = df.loc[(df['Player'] == player) & (df['Cycle '] == cycle)]\n",
    "   \n",
    "    if len(indata[indata[column]>0]) >0:\n",
    "        \n",
    "        if base>1:\n",
    "\n",
    "                start = list(filter(lambda i: i > 0, indata[column]))[0]\n",
    "                #print(start)\n",
    "    \n",
    "\n",
    "                ind = list(indata[column]).index(start)\n",
    "                #print('bases', indata[column].iloc[ind:(ind+(base+1))])\n",
    "                basel = indata[column].iloc[ind:ind+base].mean()\n",
    "                #print('baseline', basel, 'start', start, 'ind', ind, 'base', base)\n",
    "        elif base == 1:\n",
    "                #get min value above zero\n",
    "\n",
    "                \n",
    "                basel = indata[column].where(indata[column].gt(0)).min(0)\n",
    "                #print(basel)\n",
    "        df.loc[(df['Player'] == player) & (df['Cycle '] == cycle), column + '_difference'] = (df.loc[(df['Player'] == player) & (df['Cycle '] == cycle), column] / basel)*100\n",
    "\n",
    "        \n",
    "base = 5\n",
    "for d in [every3rdfull, everyotherfull, twiceaweekfull, oneperweekfull]:\n",
    "    for i in d['Player'].unique():\n",
    "        for j in d['Cycle '].unique():\n",
    "            baseline(d, i, j, 'E2', base)\n",
    "            baseline(d, i, j, 'P4', base)\n",
    "\n",
    "\n",
    "def kassam(data, player, cycle):  \n",
    "    indata = data.loc[(data['Player'] == player) & (data['Cycle '] == cycle)]  \n",
    "    #print('player:', player, 'cycle:', cycle)\n",
    "    #print(len(indata))\n",
    "    \n",
    "    # Set the initial loop index to 4 (since we are looking 4 days prior to the first day with p4_difference > 150)\n",
    "    indxlist  = indata.index.tolist()\n",
    "    #print(len(indxlist))\n",
    "    if len(indxlist)>4:\n",
    "        #print(idxlist)\n",
    "        i = indxlist[10]\n",
    "        #print('startindex', i)\n",
    "    else:\n",
    "        i = 0\n",
    "        #print(i)\n",
    "\n",
    "    # Use a flag variable to keep track of whether three consecutive days have been found\n",
    "    '''consecutive_days_found = False\n",
    "\n",
    "    # Loop through the dataframe until three consecutive days are found or the end of the data is reached\n",
    "    while (i < len(indata)) and (not consecutive_days_found):\n",
    "        # Check if the current day and the two previous days have p4_difference > 150\n",
    "        if (indata.loc[i-2:i, 'P4_difference'] > 150).all():\n",
    "            # If all three days have p4_difference > 150, set the value of Kassam four days prior to the start index to 1\n",
    "            data.loc[i-6, 'Kassam'] = 1\n",
    "            # Set the flag variable to True to indicate that three consecutive days have been found\n",
    "            consecutive_days_found = True\n",
    "            break\n",
    "            #print('Kassam found for ' + str(player) + ' on cycle ' + str(cycle))\n",
    "        else:\n",
    "            # If three consecutive days are not found, move on to the next day\n",
    "            i += 1'''\n",
    "    '''for i in range(len(indata)-2):\n",
    "        if indata.iloc[i]['P4_difference'] > 150 and indata.iloc[i+1]['p4_difference'] > 150 and indata.iloc[i+2]['p4_difference'] > 150:\n",
    "            return i\n",
    "    return -1'''\n",
    "    for j in range(i, i+(len(indata))):\n",
    "        # Check if the current day and the two previous days have p4_difference > 150\n",
    "            if ((indata.loc[j-2:j, 'P4_difference'] > 150).all()):\n",
    "                #print(indata.loc[j-2:j])\n",
    "                #print(j-6)\n",
    "                # If all three days have p4_difference > 150, set the value of Kassam four days prior to the start index to 1\n",
    "                return j-6\n",
    "                # Move the loop index forward to the day after the third consecutive day\n",
    "                #i += 1\n",
    "            else:\n",
    "                 continue\n",
    "                # If three consecutive days are not found, move on to the next day\n",
    "                #continue\n",
    "            \n",
    "\n",
    "            \n",
    "\n",
    "    # Output a message indicating whether three consecutive days were found or not\n",
    "    \n",
    "\n",
    "for d in [every3rdfull, everyotherfull, twiceaweekfull, oneperweekfull, fulldata]:\n",
    "    idxlist = []\n",
    "    for i in d['Player'].unique():\n",
    "        for j in d['Cycle '].unique():\n",
    "            #print(i, j)\n",
    "            idxlist.append(kassam(d, i, j))\n",
    "\n",
    "    idxlist = [idx for idx in idxlist if (idx != None) & (idx != -6)]\n",
    "    d.loc[idxlist, 'Kassam'] = 1\n",
    "    d['Kassam'] = d['Kassam'].fillna(0)\n",
    "    \n",
    "\n",
    "#print(every3rdfull.Kassam.head(15))\n",
    "\n",
    "every3rdfull.to_csv('C:/Users/KatherineRidley/OneDrive - Mint Diagnostics Ltd/Documents/Data Analysis/every3rdfullinterpolatedkassam.csv')\n",
    "everyotherfull.to_csv('C:/Users/KatherineRidley/OneDrive - Mint Diagnostics Ltd/Documents/Data Analysis/everyotherfullinterpolatedkassam.csv')\n",
    "twiceaweekfull.to_csv('C:/Users/KatherineRidley/OneDrive - Mint Diagnostics Ltd/Documents/Data Analysis/twiceaweekfullinterpolatedkassam.csv')\n",
    "oneperweekfull.to_csv('C:/Users/KatherineRidley/OneDrive - Mint Diagnostics Ltd/Documents/Data Analysis/oneperweekfullinterpolatedkassam.csv')\n",
    "fulldata.to_csv('C:/Users/KatherineRidley/OneDrive - Mint Diagnostics Ltd/Documents/Data Analysis/fulldatainterpolatedkassam.csv')\n",
    "\n",
    "def baird(data, player, cycle):  \n",
    "    indata = data.loc[(data['Player'] == player) & (data['Cycle '] == cycle)]  \n",
    "    #print('player:', player, 'cycle:', cycle)\n",
    "    #print(len(indata))\n",
    "    indata['E2/P4'] = indata['E2/P4'].replace(0, np.nan)\n",
    "    indata = indata.dropna(subset=['E2/P4'])\n",
    "    # Set the initial loop index to 4 (since we are looking 4 days prior to the first day with p4_difference > 150)\n",
    "    indxlist  = indata.index.tolist()\n",
    "    #print(len(indxlist))\n",
    "    #print(indxlist)\n",
    "    if len(indxlist)>4:\n",
    "        \n",
    "        i = indxlist[6]\n",
    "        #print('startindex', i)\n",
    "        count=5        \n",
    "            \n",
    "    \n",
    "    \n",
    "        #for j in range(i, i+(len(indata)-4)):\n",
    "        for j in indxlist:\n",
    "            #print(j+5)\n",
    "            \n",
    "            newlist = indxlist[count:count+5]\n",
    "            #print(newlist)\n",
    "            #print('begin loop at', newlist[0])\n",
    "            #diff = indata.iloc[j+4]['E2/P4'] - indata.iloc[j]['E2/P4']\n",
    "            \n",
    "            #if diff >= (0.6 * indata.iloc[j]['E2/P4']):\n",
    "            #indata['E2/P4'] = indata['E2/P4'].fillna(0)\n",
    "            \n",
    "            #print(indata['E2/P4'])\n",
    "            #print(newlist[4], newlist[0])\n",
    "            #print(indata.iloc[newlist[4]])\n",
    "            #print(indata.index.tolist())\n",
    "            if indata.loc[newlist[4]]['E2/P4']*2 <= (indata.loc[newlist[0]]['E2/P4']):\n",
    "                #print(indata.loc[newlist[4]]['E2/P4'], indata.loc[newlist[0]]['E2/P4'])\n",
    "                #print('Index', newlist[1])\n",
    "                #print('Day', indata.loc[newlist[1]]['Day'])\n",
    "                return newlist[1]\n",
    "            else:\n",
    "                count+=1\n",
    "                continue\n",
    "        return -1\n",
    "\n",
    "\n",
    "for d in [every3rdfull, everyotherfull, twiceaweekfull, oneperweekfull, fulldata]:\n",
    "    idxlist = []\n",
    "    for i in d['Player'].unique():\n",
    "        for j in d['Cycle '].unique():\n",
    "            #print(i, j)\n",
    "            idxlist.append(baird(d, i, j))\n",
    "\n",
    "    idxlist = [idx for idx in idxlist if (idx != None) & (idx != -6)]\n",
    "    d.loc[idxlist, 'Baird'] = 1\n",
    "    d['Baird'] = d['Baird'].fillna(0)\n",
    "\n",
    "every3rdfull.to_csv('C:/Users/KatherineRidley/OneDrive - Mint Diagnostics Ltd/Documents/Data Analysis/every3rdfullinterpolatedkassambaird.csv')\n",
    "everyotherfull.to_csv('C:/Users/KatherineRidley/OneDrive - Mint Diagnostics Ltd/Documents/Data Analysis/everyotherfullinterpolatedkassambaird.csv')\n",
    "twiceaweekfull.to_csv('C:/Users/KatherineRidley/OneDrive - Mint Diagnostics Ltd/Documents/Data Analysis/twiceaweekfullinterpolatedkassambaird.csv')\n",
    "oneperweekfull.to_csv('C:/Users/KatherineRidley/OneDrive - Mint Diagnostics Ltd/Documents/Data Analysis/oneperweekfullinterpolatedkassambaird.csv')\n",
    "fulldata.to_csv('C:/Users/KatherineRidley/OneDrive - Mint Diagnostics Ltd/Documents/Data Analysis/fulldatainterpolatedkassambaird.csv')\n",
    "\n",
    "## transpose\n",
    "\n",
    "##\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadict ={'every3rd':every3rdfull, 'everyother':everyotherfull, 'twiceaweek':twiceaweekfull, 'oneperweek':oneperweekfull, 'full':fulldata}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mgmddict = {'Frequency':[], 'Player':[], 'Cycle':[], 'Countback Day':[], 'Baird Day':[], 'Kassam Day':[], 'Baird MGMD':[], 'Kassam MGMD':[]}\n",
    "\n",
    "for k, v in datadict.items():\n",
    "    print(k)\n",
    "    print('Mean Kassam day', v.loc[(v['Kassam'] == 1)]['Day'].mean())\n",
    "    print('Mean Baird day', v.loc[(v['Baird'] == 1)]['Day'].mean())\n",
    "    print('Mean Ovulation countback day', v.loc[(v['Ovulation - countback'] == 1)]['Day'].mean())\n",
    "    print('Mean Ovulation day', v.loc[(v['Ovulation - LH peak'] == 1)]['Day'].mean())\n",
    "    print('SD Kassam day', v.loc[(v['Kassam'] == 1)]['Day'].std())\n",
    "    print('SD Baird day', v.loc[(v['Baird'] == 1)]['Day'].std())\n",
    "    print('SD Ovulation countback day', v.loc[(v['Ovulation - countback'] == 1)]['Day'].std())\n",
    "    print('SD Ovulation day', v.loc[(v['Ovulation - LH peak'] == 1)]['Day'].std())\n",
    "    #print(k, v.P4.mean())\n",
    "    for p in v['Player'].unique():\n",
    "        for c in v['Cycle '].unique():\n",
    "            #print(k, p, c)\n",
    "            \n",
    "            #print(k, p, c)\n",
    "            \n",
    "            if len(v.loc[(v['Player'] == p) & (v['Cycle '] == c)]) > 0:\n",
    "                mgmddict['Frequency'].append(k)\n",
    "                mgmddict['Player'].append(p)\n",
    "                mgmddict['Cycle'].append(c)\n",
    "\n",
    "                if len(v.loc[(v['Player'] == p) & (v['Cycle '] == c) & (v['Ovulation - countback'] == 1)]) > 0:\n",
    "                    mgmddict['Countback Day'].append(v.loc[(v['Player'] == p) & (v['Cycle '] == c) & (v['Ovulation - countback'] == 1)]['Day'].values[0])\n",
    "                else:\n",
    "                    mgmddict['Countback Day'].append(np.nan)\n",
    "                if len(v.loc[(v['Player'] == p) & (v['Cycle '] == c) & (v['Baird'] == 1)]) > 0:\n",
    "                    mgmddict['Baird Day'].append(v.loc[(v['Player'] == p) & (v['Cycle '] == c) & (v['Baird'] == 1)]['Day'].values[0])\n",
    "                else:\n",
    "                    mgmddict['Baird Day'].append(np.nan)\n",
    "                if len(v.loc[(v['Player'] == p) & (v['Cycle '] == c) & (v['Kassam'] == 1)]) > 0:\n",
    "                    mgmddict['Kassam Day'].append(v.loc[(v['Player'] == p) & (v['Cycle '] == c) & (v['Kassam'] == 1)]['Day'].values[0])\n",
    "                else:\n",
    "                    mgmddict['Kassam Day'].append(np.nan)\n",
    "                if len(v.loc[(v['Player'] == p) & (v['Cycle '] == c) & (v['Baird'] == 1)]) > 0:\n",
    "                    mgmddict['Baird MGMD'].append((v.loc[(v['Player'] == p) & (v['Cycle '] == c) & (v['Baird'] == 1)]['Day'].values[0]) - (v.loc[(v['Player'] == p) & (v['Cycle '] == c) & (v['Ovulation - countback'] == 1)]['Day'].values[0]))\n",
    "                else:\n",
    "                    mgmddict['Baird MGMD'].append(np.nan)\n",
    "                if (len(v.loc[(v['Player'] == p) & (v['Cycle '] == c) & (v['Kassam'] == 1)]) > 0) & (len(v.loc[(v['Player'] == p) & (v['Cycle '] == c) & (v['Ovulation - countback'] == 1)]) > 0):\n",
    "                    mgmddict['Kassam MGMD'].append((v.loc[(v['Player'] == p) & (v['Cycle '] == c) & (v['Kassam'] == 1)]['Day'].values[0]) - (v.loc[(v['Player'] == p) & (v['Cycle '] == c) & (v['Ovulation - countback'] == 1)]['Day'].values[0]))\n",
    "                else:\n",
    "                    mgmddict['Kassam MGMD'].append(np.nan)\n",
    "\n",
    "         \n",
    "mgmddata = pd.DataFrame(mgmddict)\n",
    "\n",
    "mgmddata.to_csv('C:/Users/KatherineRidley/OneDrive - Mint Diagnostics Ltd/Documents/Data Analysis/mgmddata_int.csv')\n",
    "\n",
    "print(mgmddata)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dailydict = {'Method':[], 'Frequency':[], 'Kassam':[], 'Baird':[]}\n",
    "\n",
    "for freq in mgmddata.Frequency.unique():\n",
    "    \n",
    "    dailydict['Method'].append('MGMD (SD)')\n",
    "    dailydict['Frequency'].append(freq)\n",
    "    dailydict['Kassam'].append((mgmddata.loc[(mgmddata['Frequency'] == freq)]['Kassam MGMD'].mean(), mgmddata.loc[(mgmddata['Frequency'] == freq)]['Kassam MGMD'].std()))\n",
    "    dailydict['Baird'].append((mgmddata.loc[(mgmddata['Frequency'] == freq)]['Baird MGMD'].mean(), mgmddata.loc[(mgmddata['Frequency'] == freq)]['Baird MGMD'].std()))\n",
    "\n",
    "    dailydict['Method'].append('% +- 2 days')\n",
    "    dailydict['Frequency'].append(freq)\n",
    "    dailydict['Kassam'].append((len(mgmddata.loc[(mgmddata['Frequency'] == freq) & (mgmddata['Kassam MGMD'] >= -2) & (mgmddata['Kassam MGMD'] <= 2)])/14)*100)\n",
    "    dailydict['Baird'].append((len(mgmddata.loc[(mgmddata['Frequency'] == freq) & (mgmddata['Baird MGMD'] >= -2) & (mgmddata['Baird MGMD'] <= 2)])/14)*100)\n",
    "\n",
    "    dailydict['Method'].append('% +- 4 days')\n",
    "\n",
    "    dailydict['Frequency'].append(freq)\n",
    "    dailydict['Kassam'].append((len(mgmddata.loc[(mgmddata['Frequency'] == freq) & (mgmddata['Kassam MGMD'] >= -4) & (mgmddata['Kassam MGMD'] <= 4)])/14)*100)\n",
    "    dailydict['Baird'].append((len(mgmddata.loc[(mgmddata['Frequency'] == freq) &  (mgmddata['Baird MGMD'] >= -4) & (mgmddata['Baird MGMD'] <= 4)])/14)*100)\n",
    "\n",
    "    dailydict['Method'].append('% +- 6 days')\n",
    "    dailydict['Frequency'].append(freq)\n",
    "    dailydict['Kassam'].append((len(mgmddata.loc[(mgmddata['Frequency'] == freq) & (mgmddata['Kassam MGMD'] >= -6) & (mgmddata['Kassam MGMD'] <= 6)])/14)*100)\n",
    "    dailydict['Baird'].append((len(mgmddata.loc[(mgmddata['Frequency'] == freq) & (mgmddata['Baird MGMD'] >= -6) & (mgmddata['Baird MGMD'] <= 6)])/14)*100)\n",
    "\n",
    "dailydf = pd.DataFrame(dailydict)\n",
    "\n",
    "print(dailydf)\n",
    "\n",
    "dailydf.to_csv('C:/Users/KatherineRidley/OneDrive - Mint Diagnostics Ltd/Documents/Data Analysis/MGMDdata_interpolated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadict ={'every3rd':every3rdfull, 'everyother':everyotherfull, 'twiceaweek':twiceaweekfull, 'oneperweek':oneperweekfull, 'full':fulldata}\n",
    "\n",
    "mgmddictnonint = {'Frequency':[], 'Player':[], 'Cycle':[], 'Countback Day':[], 'Baird Day':[], 'Kassam Day':[], 'Baird MGMD':[], 'Kassam MGMD':[]}\n",
    "\n",
    "for k, v in datadict.items():\n",
    "    for p in v['Player'].unique():\n",
    "        for c in v['Cycle '].unique():\n",
    "            #print(k, p, c)\n",
    "            \n",
    "            if len(v.loc[(v['Player'] == p) & (v['Cycle '] == c)]) > 0:\n",
    "                mgmddictnonint['Frequency'].append(k)\n",
    "                mgmddictnonint['Player'].append(p)\n",
    "                mgmddictnonint['Cycle'].append(c)\n",
    "\n",
    "                if len(v.loc[(v['Player'] == p) & (v['Cycle '] == c) & (v['Ovulation - countback'] == 1)]) > 0:\n",
    "                    mgmddictnonint['Countback Day'].append(v.loc[(v['Player'] == p) & (v['Cycle '] == c) & (v['Ovulation - countback'] == 1)]['Day'].values[0])\n",
    "                else:\n",
    "                    mgmddictnonint['Countback Day'].append(np.nan)\n",
    "                if len(v.loc[(v['Player'] == p) & (v['Cycle '] == c) & (v['Ovulation - Baird DLT'] == 1)]) > 0:\n",
    "                    mgmddictnonint['Baird Day'].append(v.loc[(v['Player'] == p) & (v['Cycle '] == c) & (v['Ovulation - Baird DLT'] == 1)]['Day'].values[0])\n",
    "                else:\n",
    "                    mgmddictnonint['Baird Day'].append(np.nan)\n",
    "                if len(v.loc[(v['Player'] == p) & (v['Cycle '] == c) & (v['Ovulation - Kassam'] == 1)]) > 0:\n",
    "                    mgmddictnonint['Kassam Day'].append(v.loc[(v['Player'] == p) & (v['Cycle '] == c) & (v['Ovulation - Kassam'] == 1)]['Day'].values[0] - 4)\n",
    "                else:\n",
    "                    mgmddictnonint['Kassam Day'].append(np.nan)\n",
    "                if len(v.loc[(v['Player'] == p) & (v['Cycle '] == c) & (v['Ovulation - Baird DLT'] == 1)]) > 0:\n",
    "                    mgmddictnonint['Baird MGMD'].append((v.loc[(v['Player'] == p) & (v['Cycle '] == c) & (v['Ovulation - Baird DLT'] == 1)]['Day'].values[0]) - (v.loc[(v['Player'] == p) & (v['Cycle '] == c) & (v['Ovulation - countback'] == 1)]['Day'].values[0]))\n",
    "                else:\n",
    "                    mgmddictnonint['Baird MGMD'].append(np.nan)\n",
    "                if (len(v.loc[(v['Player'] == p) & (v['Cycle '] == c) & (v['Kassam'] == 1)]) > 0) & (len(v.loc[(v['Player'] == p) & (v['Cycle '] == c) & (v['Ovulation - countback'] == 1)]) > 0):\n",
    "                    mgmddictnonint['Kassam MGMD'].append((v.loc[(v['Player'] == p) & (v['Cycle '] == c) & (v['Ovulation - Kassam'] == 1)]['Day'].values[0] - 4) - (v.loc[(v['Player'] == p) & (v['Cycle '] == c) & (v['Ovulation - countback'] == 1)]['Day'].values[0]))\n",
    "                else:\n",
    "                    mgmddictnonint['Kassam MGMD'].append(np.nan)\n",
    "\n",
    "#print(mgmddictnonint)\n",
    "mgmddatanonint = pd.DataFrame(mgmddictnonint)\n",
    "\n",
    "mgmddatanonint.to_csv('C:/Users/KatherineRidley/OneDrive - Mint Diagnostics Ltd/Documents/Data Analysis/mgmddatanonint.csv')\n",
    "\n",
    "print(mgmddatanonint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dailydict = {'Method':[], 'Frequency':[], 'Kassam':[], 'Baird':[]}\n",
    "\n",
    "for freq in mgmddata.Frequency.unique():\n",
    "    \n",
    "    dailydict['Method'].append('MGMD (SD)')\n",
    "    dailydict['Frequency'].append(freq)\n",
    "    dailydict['Kassam'].append((mgmddata.loc[(mgmddata['Frequency'] == freq)]['Kassam MGMD'].mean(), mgmddata.loc[(mgmddata['Frequency'] == freq)]['Kassam MGMD'].std()))\n",
    "    dailydict['Baird'].append((mgmddata.loc[(mgmddata['Frequency'] == freq)]['Baird MGMD'].mean(), mgmddata.loc[(mgmddata['Frequency'] == freq)]['Baird MGMD'].std()))\n",
    "\n",
    "    dailydict['Method'].append('% +- 2 days')\n",
    "    dailydict['Frequency'].append(freq)\n",
    "    dailydict['Kassam'].append((len(mgmddata.loc[(mgmddata['Frequency'] == freq) & (mgmddata['Kassam MGMD'] >= -2) & (mgmddata['Kassam MGMD'] <= 2)])/len(mgmddata.loc[(mgmddata['Frequency'] == freq)]))*100)\n",
    "    dailydict['Baird'].append((len(mgmddata.loc[(mgmddata['Frequency'] == freq) & (mgmddata['Baird MGMD'] >= -2) & (mgmddata['Baird MGMD'] <= 2)])/len(mgmddata.loc[(mgmddata['Frequency'] == freq)]))*100)\n",
    "\n",
    "    dailydict['Method'].append('% +- 4 days')\n",
    "\n",
    "    dailydict['Frequency'].append(freq)\n",
    "    dailydict['Kassam'].append((len(mgmddata.loc[(mgmddata['Frequency'] == freq) & (mgmddata['Kassam MGMD'] >= -4) & (mgmddata['Kassam MGMD'] <= 4)])/len(mgmddata.loc[(mgmddata['Frequency'] == freq)]))*100)\n",
    "    dailydict['Baird'].append((len(mgmddata.loc[(mgmddata['Frequency'] == freq) &  (mgmddata['Baird MGMD'] >= -4) & (mgmddata['Baird MGMD'] <= 4)])/len(mgmddata.loc[(mgmddata['Frequency'] == freq)]))*100)\n",
    "\n",
    "dailydf = pd.DataFrame(dailydict)\n",
    "\n",
    "print(dailydf)\n",
    "\n",
    "dailydf.to_csv('C:/Users/KatherineRidley/OneDrive - Mint Diagnostics Ltd/Documents/Data Analysis/MGMDdata_interpolated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_with_NaN(df, col):\n",
    "    df2 = df.copy()\n",
    "    columns = ['E2', 'P4', 'E2_difference', 'P4_difference']\n",
    "    for column in columns:\n",
    "        \n",
    "        df2[column] = np.where(df2[col] != 1, np.nan, df2[column])\n",
    "    return df2\n",
    "\n",
    "#print(data)\n",
    "\n",
    "cols = ['Every 7th day', 'Every other day', 'Every 3rd day', 'Twice a week']\n",
    "\n",
    "every3rdfull = replace_with_NaN(data, 'Every 3rd day')\n",
    "everyotherfull = replace_with_NaN(data, 'Every other day')\n",
    "twiceaweekfull = replace_with_NaN(data, 'Twice a week')\n",
    "oneperweekfull = replace_with_NaN(data, 'Every 7th day')\n",
    "\n",
    "\n",
    "\n",
    "dataall = {'d': data, 'baseline': 5, 'kassam':3, 'baird':5, 'name': 'All'}\n",
    "everyother = {'d': everyotherfull, 'baseline': 3, 'kassam':2, 'baird':3, 'name': 'Every other day'}\n",
    "every3rd = {'d': every3rdfull, 'baseline': 1, 'kassam':1, 'baird':3, 'name': 'Every 3rd day'}\n",
    "twiceaweek = {'d': twiceaweekfull, 'baseline': 1, 'kassam':1, 'baird':3, 'name': 'Twice a week'}\n",
    "oneperweek = {'d': oneperweekfull, 'baseline': 1, 'kassam':1, 'baird':2, 'name': 'Every 7th day'}\n",
    "\n",
    "def baseline(df, player, cycle, column, base):\n",
    "    indata = df.loc[(df['Player'] == player) & (df['Cycle '] == cycle)]\n",
    "   \n",
    "    if len(indata[indata[column]>0]) >0:\n",
    "        \n",
    "        if base>1:\n",
    "\n",
    "                start = list(filter(lambda i: i > 0, indata[column]))[0]\n",
    "                #print(start)\n",
    "    \n",
    "\n",
    "                ind = list(indata[column]).index(start)\n",
    "                #print('bases', indata[column].iloc[ind:(ind+(base+1))])\n",
    "                basel = indata[column].iloc[ind:ind+base].mean()\n",
    "                #print('baseline', basel, 'start', start, 'ind', ind, 'base', base)\n",
    "        elif base == 1:\n",
    "                #get min value above zero\n",
    "\n",
    "                \n",
    "                basel = indata[column].where(indata[column].gt(0)).min(0)\n",
    "                #print(basel)\n",
    "        df.loc[(df['Player'] == player) & (df['Cycle '] == cycle), column + '_difference'] = (df.loc[(df['Player'] == player) & (df['Cycle '] == cycle), column] / basel)*100\n",
    "\n",
    "        \n",
    "\n",
    "for d in [dataall, everyother, every3rd, twiceaweek, oneperweek]:\n",
    "    dt = d['d']\n",
    "    base = d['baseline']\n",
    "\n",
    "    for i in dt['Player'].unique():\n",
    "        for j in dt['Cycle '].unique():\n",
    "            baseline(dt, i, j, 'E2', base)\n",
    "            baseline(dt, i, j, 'P4', base)\n",
    "\n",
    "\n",
    "def kassam(data, player, cycle, kass):\n",
    "    if (cycle != 0) & (player != 0):\n",
    "        indata = data.loc[(data['Player'] == player) & (data['Cycle '] == cycle)]  \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "        # Set the initial loop index to 4 (since we are looking 4 days prior to the first day with p4_difference > 150)\n",
    "        indxlist  = indata.index.tolist()\n",
    "\n",
    "        if kass == 3:\n",
    "        #print(len(indxlist))\n",
    "            if len(indxlist)>4:\n",
    "                #print(idxlist)\n",
    "                i = indxlist[18]\n",
    "                #print('startindex', i)\n",
    "            else:\n",
    "                i = 18\n",
    "                #print(i)\n",
    "\n",
    "            # Use a flag variable to keep track of whether three consecutive days have been found\n",
    "        \n",
    "            for j in range(i, i+(len(indata)-18)):\n",
    "                # Check if the current day and the two previous days have p4_difference > 150\n",
    "                    if ((indata.loc[j-2:j, 'P4_difference'] > 150).all()):\n",
    "                        data.loc[j-2:j, 'Kassam+4'] = 1\n",
    "                        #print(indata.loc[j-2:j])\n",
    "                        #print(j-6)\n",
    "                        # If all three days have p4_difference > 150, set the value of Kassam four days prior to the start index to 1\n",
    "                        return j-6\n",
    "                    # Move the loop index forward to the day after the third consecutive day\n",
    "                    #i += 1\n",
    "                    else:\n",
    "                        continue\n",
    "                        # If three consecutive days are not found, move on to the next day\n",
    "                        #continue\n",
    "                \n",
    "        elif kass == 2:\n",
    "            #print(len(indxlist))\n",
    "            if len(indxlist)>1:\n",
    "                if len(indxlist)>4:\n",
    "                    #print(idxlist)\n",
    "                    i = indxlist[18]\n",
    "                    #print('startindex', i)\n",
    "                else:\n",
    "                    i = 18\n",
    "                    \n",
    "                    #print(i)\n",
    "\n",
    "                # Use a flag variable to keep track of whether three consecutive days have been found\n",
    "\n",
    "                for j in range(i, i+((len(indata))-18)):\n",
    "                        #print(j)\n",
    "                        \n",
    "                        \n",
    "                        if (len(indata.loc[j])>0) and (len(indata.loc[j-2])>0):\n",
    "                        #if (indata.loc[j]['P4'] == np.nan) or (indata.loc[j]['P4'] == 0) or (indata.loc[j-2]['P4'] == np.nan) or (indata.loc[j-2]['P4'] == 0):\n",
    "\n",
    "                           # continue\n",
    "                        \n",
    "\n",
    "                    # Check if the current day and the two previous days have p4_difference > 150\n",
    "                        \n",
    "                            if (indata.loc[j-2, 'P4_difference'] > 150) and (indata.loc[j, 'P4_difference'] > 150):\n",
    "                                data.loc[j-2:j, 'Kassam+4'] = 1\n",
    "                                #print(indata.loc[j-2:j, 'P4_difference'])\n",
    "                                #print(indata.loc[j-2:j])\n",
    "                                #print(j-6)\n",
    "                                # If all three days have p4_difference > 150, set the value of Kassam four days prior to the start index to 1\n",
    "                                return j-6\n",
    "                                break\n",
    "                            # Move the loop index forward to the day after the third consecutive day\n",
    "                            #i += 1\n",
    "                            else:\n",
    "                                \n",
    "                                continue\n",
    "                \n",
    "        elif kass == 1:\n",
    "            \n",
    "            if len(indxlist)>4:\n",
    "                #print(idxlist)\n",
    "                i = indxlist[16]\n",
    "                #print('startindex', i)\n",
    "            else:\n",
    "                i = 16\n",
    "                #print(i)\n",
    "\n",
    "            # Use a flag variable to keep track of whether three consecutive days have been found\n",
    "        \n",
    "            for j in range(i-1, i+(len(indata))-18):\n",
    "                    if (indata.loc[j]['P4'] == np.nan) or (indata.loc[j]['P4'] == 0) or (indata.loc[j-2]['P4'] == np.nan) or (indata.loc[j-2]['P4'] == 0):\n",
    "\n",
    "                            continue\n",
    "                # Check if the current day and the two previous days have p4_difference > 150\n",
    "                    elif (indata.loc[j, 'P4_difference'] > 150):\n",
    "                        data.loc[j-1:j+1, 'Kassam+4'] = 1\n",
    "                        #print(indata.loc[j-2:j])\n",
    "                        #print(j-6)\n",
    "                        # If all three days have p4_difference > 150, set the value of Kassam four days prior to the start index to 1\n",
    "                        return j-4\n",
    "                        break\n",
    "                    # Move the loop index forward to the day after the third consecutive day\n",
    "                    #i += 1\n",
    "                    else:\n",
    "                        continue\n",
    "            \n",
    "\n",
    "    # Output a message indicating whether three consecutive days were found or not\n",
    "    \n",
    "\n",
    "for d in [dataall, everyother, every3rd, twiceaweek, oneperweek]:\n",
    "    idxlist = []\n",
    "    dt = d['d']\n",
    "    kass = d['kassam']\n",
    "    print(d['name'])\n",
    "    #print(idxlist)\n",
    "    #print(dt.index.to_list())\n",
    "    for i in dt['Player'].unique():\n",
    "        for j in dt['Cycle '].unique():\n",
    "            print(i, j)\n",
    "            idxlist.append(kassam(dt, i, j, kass))\n",
    "\n",
    "    idxlist = [idx for idx in idxlist if (idx != None) & (idx != -6) & (idx != -2) & (idx != -4) & (idx != -1)]\n",
    "    \n",
    "    dt.loc[idxlist, 'Kassam'] = 1\n",
    "    dt['Kassam'] = dt['Kassam'].fillna(0)\n",
    "    \n",
    "\n",
    "#print(every3rdfull.Kassam.head(15))\n",
    "\n",
    "#every3rdfull.to_csv('C:/Users/KatherineRidley/OneDrive - Mint Diagnostics Ltd/Documents/Data Analysis/every3rdfullinterpolatedkassam.csv')\n",
    "#everyotherfull.to_csv('C:/Users/KatherineRidley/OneDrive - Mint Diagnostics Ltd/Documents/Data Analysis/everyotherfullinterpolatedkassam.csv')\n",
    "#twiceaweekfull.to_csv('C:/Users/KatherineRidley/OneDrive - Mint Diagnostics Ltd/Documents/Data Analysis/twiceaweekfullinterpolatedkassam.csv')\n",
    "#oneperweekfull.to_csv('C:/Users/KatherineRidley/OneDrive - Mint Diagnostics Ltd/Documents/Data Analysis/oneperweekfullinterpolatedkassam.csv')\n",
    "#fulldata.to_csv('C:/Users/KatherineRidley/OneDrive - Mint Diagnostics Ltd/Documents/Data Analysis/fulldatainterpolatedkassam.csv')\n",
    "\n",
    "def baird(data, player, cycle, baird): \n",
    "    if (cycle != 0) & (player != 0):\n",
    "        indata = data.loc[(data['Player'] == player) & (data['Cycle '] == cycle)]  \n",
    "        \n",
    "        indata['E2/P4'] = indata['E2/P4'].replace(0, np.nan)\n",
    "        indata = indata.dropna(subset=['E2/P4'])\n",
    "        \n",
    "        indxlist  = indata.index.tolist()\n",
    "        \n",
    "        if len(indxlist)>4:\n",
    "            \n",
    "            i = indxlist[10]\n",
    "            #print('startindex', i)\n",
    "            count=10      \n",
    "                \n",
    "        \n",
    "            for j in indxlist:\n",
    "                \n",
    "                \n",
    "                newlist = indxlist[count:count+baird]\n",
    "                #print(baird, baird-1)\n",
    "                if len(newlist) < baird:\n",
    "                    continue\n",
    "                elif indata.loc[newlist[baird-1]]['E2/P4']*2 <= (indata.loc[newlist[0]]['E2/P4']):\n",
    "                    data.loc[newlist[0]:newlist[baird-1], 'Bairdrange'] = 1\n",
    "                    if baird == 5:\n",
    "                        return newlist[1]\n",
    "                    else:\n",
    "                        return newlist[0]\n",
    "                \n",
    "                    \n",
    "                else:\n",
    "                    count+=1\n",
    "                    continue\n",
    "        return None\n",
    "\n",
    "\n",
    "for d in [dataall, everyother, every3rd, twiceaweek, oneperweek]:\n",
    "    idxlist = []\n",
    "    dt = d['d']\n",
    "    idxlist = []\n",
    "    print(d['name'])\n",
    "    for i in dt['Player'].unique():\n",
    "        for j in dt['Cycle '].unique():\n",
    "\n",
    "            #print(i, j)\n",
    "            \n",
    "            idxlist.append(baird(dt, i, j, d['baird']))\n",
    "\n",
    "    idxlist = [idx for idx in idxlist if (idx != None) & (idx != -6)]\n",
    "    dt.loc[idxlist, 'Baird'] = 1\n",
    "    dt['Baird'] = dt['Baird'].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadict ={'every3rd':every3rd['d'], 'everyother':everyother['d'], 'oneperweek':oneperweek['d'], 'full':dataall['d']}\n",
    "\n",
    "print(every3rd['d'].tail(20))\n",
    "print(every3rd['d']['P4_difference'].tail(20))\n",
    "\n",
    "\n",
    "\n",
    "mgmddict = {'Frequency':[], 'Player':[], 'Cycle':[], 'Countback Day':[], 'Baird Day':[], 'Kassam Day':[], 'Baird MGMD':[], 'Kassam MGMD':[]}\n",
    "\n",
    "for k, v in datadict.items():\n",
    "    print(k)\n",
    "    print('Mean Kassam day', v.loc[(v['Kassam'] == 1)]['Day'].mean())\n",
    "    print('Mean Baird day', v.loc[(v['Baird'] == 1)]['Day'].mean())\n",
    "    print('Mean Ovulation countback day', v.loc[(v['Ovulation - countback'] == 1)]['Day'].mean())\n",
    "    print('Mean Ovulation day', v.loc[(v['Ovulation - LH peak'] == 1)]['Day'].mean())\n",
    "    print('SD Kassam day', v.loc[(v['Kassam'] == 1)]['Day'].std())\n",
    "    print('SD Baird day', v.loc[(v['Baird'] == 1)]['Day'].std())\n",
    "    print('SD Ovulation countback day', v.loc[(v['Ovulation - countback'] == 1)]['Day'].std())\n",
    "    print('SD Ovulation day', v.loc[(v['Ovulation - LH peak'] == 1)]['Day'].std())\n",
    "    #print(k, v.P4.mean())\n",
    "    for p in v['Player'].unique():\n",
    "        for c in v['Cycle '].unique():\n",
    "            #print(k, p, c)\n",
    "            \n",
    "            #print(k, p, c)\n",
    "            \n",
    "            if len(v.loc[(v['Player'] == p) & (v['Cycle '] == c)]) > 0:\n",
    "                mgmddict['Frequency'].append(k)\n",
    "                mgmddict['Player'].append(p)\n",
    "                mgmddict['Cycle'].append(c)\n",
    "\n",
    "                if len(v.loc[(v['Player'] == p) & (v['Cycle '] == c) & (v['Ovulation - countback'] == 1)]) > 0:\n",
    "                    mgmddict['Countback Day'].append(v.loc[(v['Player'] == p) & (v['Cycle '] == c) & (v['Ovulation - countback'] == 1)]['Day'].values[0])\n",
    "                else:\n",
    "                    mgmddict['Countback Day'].append(np.nan)\n",
    "                if len(v.loc[(v['Player'] == p) & (v['Cycle '] == c) & (v['Baird'] == 1)]) > 0:\n",
    "                    mgmddict['Baird Day'].append(v.loc[(v['Player'] == p) & (v['Cycle '] == c) & (v['Baird'] == 1)]['Day'].values[0])\n",
    "                else:\n",
    "                    mgmddict['Baird Day'].append(np.nan)\n",
    "                if len(v.loc[(v['Player'] == p) & (v['Cycle '] == c) & (v['Kassam'] == 1)]) > 0:\n",
    "                    mgmddict['Kassam Day'].append(v.loc[(v['Player'] == p) & (v['Cycle '] == c) & (v['Kassam'] == 1)]['Day'].values[0])\n",
    "                else:\n",
    "                    mgmddict['Kassam Day'].append(np.nan)\n",
    "                if len(v.loc[(v['Player'] == p) & (v['Cycle '] == c) & (v['Baird'] == 1)]) > 0:\n",
    "                    mgmddict['Baird MGMD'].append((v.loc[(v['Player'] == p) & (v['Cycle '] == c) & (v['Baird'] == 1)]['Day'].values[0]) - (v.loc[(v['Player'] == p) & (v['Cycle '] == c) & (v['Ovulation - countback'] == 1)]['Day'].values[0]))\n",
    "                else:\n",
    "                    mgmddict['Baird MGMD'].append(np.nan)\n",
    "                if (len(v.loc[(v['Player'] == p) & (v['Cycle '] == c) & (v['Kassam'] == 1)]) > 0) & (len(v.loc[(v['Player'] == p) & (v['Cycle '] == c) & (v['Ovulation - countback'] == 1)]) > 0):\n",
    "                    mgmddict['Kassam MGMD'].append((v.loc[(v['Player'] == p) & (v['Cycle '] == c) & (v['Kassam'] == 1)]['Day'].values[0]) - (v.loc[(v['Player'] == p) & (v['Cycle '] == c) & (v['Ovulation - countback'] == 1)]['Day'].values[0]))\n",
    "                else:\n",
    "                    mgmddict['Kassam MGMD'].append(np.nan)\n",
    "\n",
    "         \n",
    "mgmddata = pd.DataFrame(mgmddict)\n",
    "\n",
    "mgmddata.to_csv('C:/Users/KatherineRidley/OneDrive - Mint Diagnostics Ltd/Documents/Data Analysis/mgmddata_nonint.csv')\n",
    "\n",
    "print(mgmddata)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dailydict = {'Method':[], 'Frequency':[], 'Kassam':[], 'Baird':[]}\n",
    "\n",
    "for freq in mgmddata.Frequency.unique():\n",
    "    \n",
    "    dailydict['Method'].append('MGMD (SD)')\n",
    "    dailydict['Frequency'].append(freq)\n",
    "    dailydict['Kassam'].append((mgmddata.loc[(mgmddata['Frequency'] == freq)]['Kassam MGMD'].mean(), mgmddata.loc[(mgmddata['Frequency'] == freq)]['Kassam MGMD'].std()))\n",
    "    dailydict['Baird'].append((mgmddata.loc[(mgmddata['Frequency'] == freq)]['Baird MGMD'].mean(), mgmddata.loc[(mgmddata['Frequency'] == freq)]['Baird MGMD'].std()))\n",
    "\n",
    "    dailydict['Method'].append('% +- 2 days')\n",
    "    dailydict['Frequency'].append(freq)\n",
    "    dailydict['Kassam'].append((len(mgmddata.loc[(mgmddata['Frequency'] == freq) & (mgmddata['Kassam MGMD'] >= -2) & (mgmddata['Kassam MGMD'] <= 2)])/14)*100)\n",
    "    dailydict['Baird'].append((len(mgmddata.loc[(mgmddata['Frequency'] == freq) & (mgmddata['Baird MGMD'] >= -2) & (mgmddata['Baird MGMD'] <= 2)])/14)*100)\n",
    "\n",
    "    dailydict['Method'].append('% +- 4 days')\n",
    "\n",
    "    dailydict['Frequency'].append(freq)\n",
    "    dailydict['Kassam'].append((len(mgmddata.loc[(mgmddata['Frequency'] == freq) & (mgmddata['Kassam MGMD'] >= -4) & (mgmddata['Kassam MGMD'] <= 4)])/14)*100)\n",
    "    dailydict['Baird'].append((len(mgmddata.loc[(mgmddata['Frequency'] == freq) &  (mgmddata['Baird MGMD'] >= -4) & (mgmddata['Baird MGMD'] <= 4)])/14)*100)\n",
    "\n",
    "    dailydict['Method'].append('Ovulation detected (%)')\n",
    "    dailydict['Frequency'].append(freq)\n",
    "    dailydict['Kassam'].append((len(mgmddata.loc[(mgmddata['Frequency'] == freq) & (mgmddata['Kassam MGMD'] >= -14) & (mgmddata['Kassam MGMD'] <= 14)])/14)*100)\n",
    "    dailydict['Baird'].append((len(mgmddata.loc[(mgmddata['Frequency'] == freq) & (mgmddata['Baird MGMD'] >= -14) & (mgmddata['Baird MGMD'] <= 14)])/14)*100)\n",
    "\n",
    "dailydf = pd.DataFrame(dailydict)\n",
    "\n",
    "print(dailydf)\n",
    "\n",
    "dailydf.to_csv('C:/Users/KatherineRidley/OneDrive - Mint Diagnostics Ltd/Documents/Data Analysis/MGMDdata_noninterpolated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "datadict ={'every 3rd day':every3rd['d'], 'one per week':oneperweek['d'], 'full':dataall['d']}\n",
    "\n",
    "for k, v in datadict.items():\n",
    "        for p in v.Player.unique():\n",
    "             for c in v['Cycle '].unique():\n",
    "                value = v.loc[(v['Player']==p) & (v['Cycle ']==c)]\n",
    "                if len(value) > 5:\n",
    "                    #value = value.set_index('Day')\n",
    "                    #value = value[['P4', 'E2', 'Menses']]\n",
    "                    value = value.replace(0, np.nan)\n",
    "                    values = value.dropna(subset=['P4', 'E2'])\n",
    "                    #print(value)\n",
    "                    #value= value.interpolate(method='linear', limit_direction='forward', axis=0)\n",
    "\n",
    "                    fig, (ax1) = plt.subplots(nrows=1, ncols=1, sharex = True, figsize=(10,5))\n",
    "                    ax2 = ax1.twinx()\n",
    "                    \n",
    "                    values.reset_index().plot(ax = ax1, x='Day', y='P4', color = 'palegreen', linewidth=6, label='Progesterone Average')\n",
    "                    values.reset_index().plot(ax = ax1, x='Day', y='P4', color = 'limegreen', marker = 'o', markersize = 6, linestyle = 'none', label = 'Progesterone')\n",
    "                    values.reset_index().plot(ax = ax2, x='Day', y='E2', color = '#FFC8D3', linewidth=6, label='Oestradiol Average')\n",
    "                    values.reset_index().plot(ax = ax2, x='Day', y='E2', color = '#DB2525', marker = 'o', markersize = 6, linestyle = 'none', label = 'Oestradiol')\n",
    "                    ylow, yhigh = ax1.get_ylim()\n",
    "                    xlow, xhigh = ax1.get_xlim()\n",
    "                    for i in range(len(value.loc[value['Menses']==1])):\n",
    "                        '''if i == 0:\n",
    "                                \n",
    "                            ax1.text(xlow+((xhigh-xlow)/20)\n",
    "                            , yhigh-((yhigh-ylow)/20), \n",
    "                            'Menses', color = '#AA0E24', fontsize = 14, fontstyle = 'normal', fontweight = 'bold', horizontalalignment='left')'''\n",
    "\n",
    "                                    \n",
    "                        ax1.axvline(value.loc[value['Menses']==1, 'Day'].iloc[i], linestyle = 'None',color='crimson', marker = 'o',  markersize = 14)\n",
    "\n",
    "                    ax1.set_xlim(0, 30)\n",
    "                    ax1.axvline(value.loc[value['Ovulation - countback']==1, 'Day'].iloc[0], linestyle = '-',color='blue', marker = 'o',  markersize = 15)\n",
    "                    if len(value.loc[value['Kassam']==1])>0:\n",
    "                        ax1.axvspan(value.loc[value['Kassam']==1, 'Day'].iloc[0]-1, value.loc[value['Kassam']==1, 'Day'].iloc[0]+1, color='violet', alpha=0.2)\n",
    "                        ax1.axvline(value.loc[value['Kassam']==1, 'Day'].iloc[0], linestyle = '--',color='violet', marker = 'None', alpha=0.5)\n",
    "                    #if len(value.loc[value['Kassam+4']==1])>0:\n",
    "                       # ax1.axvspan(value.loc[value['Kassam+4']==1, 'Day'].iloc[0], value.loc[value['Kassam+4']==1, 'Day'].max() , alpha=0.2, color='violet')\n",
    "                    if len(value.loc[value['Baird']==1])>0:\n",
    "                        ax1.axvspan(value.loc[value['Baird']==1, 'Day'].iloc[0]-1, value.loc[value['Baird']==1, 'Day'].iloc[0]+1, color='orange', alpha=0.2)\n",
    "                        ax1.axvline(value.loc[value['Baird']==1, 'Day'].iloc[0], linestyle = '--',color='orange', marker = 'None', alpha=0.5)\n",
    "                    #if len(value.loc[value['Bairdrange']==1])>0:\n",
    "                        #ax1.axvspan(value.loc[value['Bairdrange']==1, 'Day'].iloc[0], value.loc[value['Bairdrange']==1, 'Day'].max() , alpha=0.2, color='orange')\n",
    "\n",
    "                    #ax1.axvline(1, linestyle = '--',color='gray', marker = 'None')\n",
    "                    ax1.axvspan(1, 5, alpha=0.2, color='gray')\n",
    "                    #ax1.axvline(5, linestyle = '--',color='gray', marker = 'None')\n",
    "\n",
    "                    \n",
    "                    \n",
    "\n",
    "                            # Set limits\n",
    "\n",
    "                    #ax1.set_xlim(value.reset_index()['Day'].min() -2, value.reset_index()['Day'].max() + 2)\n",
    "\n",
    "                    # ax1.set_ylim(0, e_df['edata'].max() + 0.5)\n",
    "\n",
    "\n",
    "                    plt.legend('', frameon=False)\n",
    "                    ax1.legend('', frameon=False)\n",
    "\n",
    "                    ax2.set_ylabel('Oestradiol, pg/mL', color = '#DB2525', fontsize = 16)\n",
    "                    ax1.set_ylabel('Progesterone, pg/mL', color = 'limegreen', fontsize = 16)\n",
    "                    ax1.set_xlabel('Cycle day', fontsize = 16)\n",
    "                    plt.title('Player '+str(p) +', Cycle '+str(c)+ ', Frequency '+str(k), fontsize = 18)\n",
    "                    \n",
    "                    \n",
    "                    # plt.grid(True, axis = 'x' )\n",
    "                    #ax1.grid(True, which = 'both', axis = 'x', color='#DDDDDD', linestyle='--', linewidth=1 )\n",
    "                    plt.savefig('{}_{}_{}.png'.format(p, c, k), format='png', dpi=1200, bbox_inches='tight', facecolor='w', transparent=False)\n",
    "                    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "datadict ={'every3rd':every3rdfull, 'everyother':everyotherfull, 'twiceaweek':twiceaweekfull, 'oneperweek':oneperweekfull, 'full':fulldata}\n",
    "\n",
    "newdict = {'Frequency':[],  'Baird TP':[], 'Baird TN':[], 'Baird FP':[], 'Baird FN':[], \n",
    "           'Kassam TP':[], 'Kassam TN':[], 'Kassam FP':[], 'Kassam FN':[], 'CB day':[], 'Kassam Day':[], 'Baird Day':[], 'Kassam Accuracy':[], 'Baird Accuracy':[], \n",
    "           'Kassam Recall':[], 'Baird Recall':[], 'Kassam Precision':[], 'Baird Precision':[],  'Kassam Specificity':[], 'Baird Specificity':[]}\n",
    "\n",
    "for k, v in datadict.items():\n",
    "     #for i in v['Player'].unique():\n",
    "        #for j in v['Cycle '].unique():\n",
    "            #newdict['Player'].append(i)\n",
    "            #newdict['Cycle'].append(j)\n",
    "            newdict['Frequency'].append(k)\n",
    "            newdict['CB day'].append(v.loc[(v['Ovulation - countback'] == 1), 'Day'].mean())\n",
    "            newdict['Kassam Day'].append(v.loc[(v['Kassam'] == 1), 'Day'].mean())\n",
    "            newdict['Baird Day'].append(v.loc[(v['Baird'] == 1), 'Day'].mean())\n",
    "            newdict['Baird TP'].append(len(v.loc[(v['Ovulation - countback'] == 1) & (v['Baird'] == 1)]))\n",
    "            newdict['Baird TN'].append(len(v.loc[(v['Ovulation - countback'] == 0) & (v['Baird'] == 0)]))\n",
    "            newdict['Baird FP'].append(len(v.loc[(v['Ovulation - countback'] == 0) & (v['Baird'] == 1)]))\n",
    "            newdict['Baird FN'].append(len(v.loc[(v['Ovulation - countback'] == 1) & (v['Baird'] == 0)]))\n",
    "            newdict['Kassam TP'].append(len(v.loc[(v['Ovulation - countback'] == 1) & (v['Kassam'] == 1)]))\n",
    "            newdict['Kassam TN'].append(len(v.loc[(v['Ovulation - countback'] == 0) & (v['Kassam'] == 0)]))\n",
    "            newdict['Kassam FP'].append(len(v.loc[(v['Ovulation - countback'] == 0) & (v['Kassam'] == 1)]))\n",
    "            newdict['Kassam FN'].append(len(v.loc[(v['Ovulation - countback'] == 1) & (v['Kassam'] == 0)]))\n",
    "\n",
    "            tp = len(v.loc[(v['Ovulation - countback'] == 1) & (v['Kassam'] == 1)])\n",
    "            tn = len(v.loc[(v['Ovulation - countback'] == 0) & (v['Kassam'] == 0)])\n",
    "            fp = len(v.loc[(v['Ovulation - countback'] == 0) & (v['Kassam'] == 1)])\n",
    "            fn = len(v.loc[(v['Ovulation - countback'] == 1) & (v['Kassam'] == 0)])\n",
    "            newdict['Kassam Recall'].append(tp/(tp+fn))\n",
    "            newdict['Kassam Precision'].append(tp/(tp+fp))\n",
    "            newdict['Kassam Specificity'].append(tn/(tn+fp))\n",
    "            newdict['Kassam Accuracy'].append((tp+tn)/(tp+tn+fp+fn))\n",
    "            tp = len(v.loc[(v['Ovulation - countback'] == 1) & (v['Baird'] == 1)])\n",
    "            tn = len(v.loc[(v['Ovulation - countback'] == 0) & (v['Baird'] == 0)])\n",
    "            fp = len(v.loc[(v['Ovulation - countback'] == 0) & (v['Baird'] == 1)])\n",
    "            fn = len(v.loc[(v['Ovulation - countback'] == 1) & (v['Baird'] == 0)])\n",
    "            newdict['Baird Recall'].append(tp/(tp+fn))\n",
    "            newdict['Baird Precision'].append(tp/(tp+fp))\n",
    "            newdict['Baird Specificity'].append(tn/(tn+fp))\n",
    "            newdict['Baird Accuracy'].append((tp+tn)/(tp+tn+fp+fn))\n",
    "\n",
    "            \n",
    "\n",
    "newdf = pd.DataFrame(newdict)\n",
    "#delete all rows where player or cycle = 0\n",
    "\n",
    "newdf.to_csv('C:/Users/KatherineRidley/OneDrive - Mint Diagnostics Ltd/Documents/Data Analysis/ovulationcountbackkassamandbaird.csv')\n",
    "\n",
    "\n",
    "accuracydict = {'Frequency':[], 'KassamAccuracy':[], 'BairdAccuracy':[], 'BairdTP':[], 'BairdFP':[], 'BairdTN':[], 'BairdFN':[], 'KassamTP':[], 'KassamFP':[], 'KassamTN':[], 'KassamFN':[]}\n",
    "\n",
    "print(newdf)\n",
    "\n",
    "for k, v in datadict.items():\n",
    "        #date diff between countback and baird\n",
    "\n",
    "        #date diff between countback and kassam\n",
    "\n",
    "        thisdict  =  {'Frequency':[], 'Player':[], 'Cycle':[], \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for each freq, compare full and 1 per week, look at interpolated data, verify method, where is predicted day, plot countback, predicted day\n",
    "\n",
    "## LOOK AT MARKER VS GOLD STANDARD, AVERAGE OF DIFFERENCE BETWEEN MARKER AND GOLD STANDARD.\n",
    "\n",
    "## literature review, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline(df, player, cycle, column, base):\n",
    "    indata = df.loc[(df['Player'] == player) & (df['Cycle '] == cycle)]\n",
    "    #print('column', column, 'base', base)\n",
    "    #print(df)\n",
    "    #indata[column] = indata[column].replace(' ', np.nan)\n",
    "    #indata.loc[column] = indata.loc[column].replace(0, np.nan, inplace=True)\n",
    "    #indata[column] = indata[column].replace(' ', np.nan)\n",
    "    #indata.loc[column] = indata.loc[column].dropna(inplace=True)\n",
    "    #print(indata.loc[indata[column]>0])\n",
    "    #print(base)\n",
    "    if len(indata[indata[column]>0]) >0:\n",
    "        \n",
    "        if base>1:\n",
    "\n",
    "                start = list(filter(lambda i: i > 0, indata[column]))[0]\n",
    "                #print(start)\n",
    "    \n",
    "\n",
    "                ind = list(indata[column]).index(start)\n",
    "                #print('bases', indata[column].iloc[ind:(ind+(base+1))])\n",
    "                basel = indata[column].iloc[ind:ind+base].mean()\n",
    "                #print('baseline', basel, 'start', start, 'ind', ind, 'base', base)\n",
    "        elif base == 1:\n",
    "                #get min value above zero\n",
    "\n",
    "                \n",
    "                basel = indata[column].where(indata[column].gt(0)).min(0)\n",
    "                #print(basel)\n",
    "        df.loc[(df['Player'] == player) & (df['Cycle '] == cycle), column + '_difference'] = (df.loc[(df['Player'] == player) & (df['Cycle '] == cycle), column] / basel)*100\n",
    "\n",
    "dataall = {'d': data, 'baseline': 5, 'kassam':4, 'baird':5}\n",
    "everyother = {'d': everyotherfull, 'baseline': 3, 'kassam':2, 'baird':3}\n",
    "every3rd = {'d': every3rdfull, 'baseline': 1, 'kassam':1, 'baird':3}\n",
    "twiceaweek = {'d': twiceaweekfull, 'baseline': 1, 'kassam':1, 'baird':3}\n",
    "oneperweek = {'d': oneperweekfull, 'baseline': 1, 'kassam':1, 'baird':2}\n",
    "\n",
    "for i in data['Player'].unique():\n",
    "\n",
    "    for j in data['Cycle '].unique():\n",
    "\n",
    "    \n",
    "\n",
    "        for d in [dataall, everyother, every3rd, twiceaweek, oneperweek]:\n",
    "            #print(i, j)\n",
    "\n",
    "\n",
    "            baseline(d['d'], i, j, 'E2', d['baseline'])\n",
    "            baseline(d['d'], i, j, 'P4', d['baseline'])\n",
    "\n",
    "print(oneperweek['d']['P4_difference'].notnull().sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolate missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_data(df, n):\n",
    "    start_date = df.iloc[0]['Date']\n",
    "    end_date = df.iloc[-1]['Date']\n",
    "    date_range = pd.date_range(start=start_date, end=end_date, freq=f'{n}D')\n",
    "    df = df.set_index('Date')\n",
    "    df = df.reindex(date_range)\n",
    "    df['Date'] = df.index\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def interpolate_data2(df):\n",
    "    start_date = df.iloc[0]['Date']\n",
    "    end_date = df.iloc[-1]['Date']\n",
    "    date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "    df = df.set_index('Date')\n",
    "    df = df.reindex(date_range, method='linear')\n",
    "    df['Date'] = df.index\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "for i in data['Player'].unique():\n",
    "\n",
    "for j in data['Cycle '].unique():\n",
    "    data = pd.read_csv('C:/Users/KatherineRidley/OneDrive - Mint Diagnostics Ltd/Documents/Data Analysis/twiceaweek.csv')\n",
    "    df = data[(data['Player'] == i) & (data['Cycle '] == j)]\n",
    "    if len(df) == 0:\n",
    "        continue\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    third = interpolate_data(df, 3)\n",
    "    print(third)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kassam method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "##interpolate missing values\n",
    "\n",
    "\n",
    "def kassammethod(data, player, cycle):\n",
    "    indata = data[(data['Player'] == player) & (data['Cycle '] == cycle)]\n",
    "    ##p4 difference >=150\n",
    "    if len(indata[indata['P4_difference']>=150]) >0:\n",
    "\n",
    "        ##get first day of p4 difference >=150\n",
    "\n",
    "        start = list(filter(lambda i: i >= 150, indata['P4_difference']))[0]\n",
    "        print(start)\n",
    "        ind = list(indata['P4_difference']).index(start)\n",
    "        \n",
    "        print(indata['Date'].iloc[ind])\n",
    "        ##make list from iloc[ind] to end of list\n",
    "        ##remove zeros\n",
    "\n",
    "        #data.loc[(data['Player'] == player) & (data['Cycle '] == cycle), 'Kassam'] = indata['Date'].iloc[ind]\n",
    "        #print(data.loc[(data['Player'] == player) & (data['Cycle '] == cycle), 'Kassam'])\n",
    "\n",
    "def mark_ovulation(df, kassam):\n",
    "    ovulation_index = None\n",
    "    for i in range(len(df) - 1):\n",
    "        for j in range(len(kassam)):\n",
    "            if df.iloc[i]['P4_Difference'] > 150 and df.iloc[i + j]['P4_Difference'] > 150:\n",
    "                ovulation_index = i\n",
    "                break\n",
    "    if ovulation_index is not None:\n",
    "        df.loc[ovulation_index - 4, 'Ovulation - Kassam'] = 1\n",
    "    return df\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##import 1 per week\n",
    "\n",
    "data1perweek = pd.read_csv('C:/Users/KatherineRidley/OneDrive - Mint Diagnostics Ltd/Documents/Data Analysis/1perweek.csv')\n",
    "\n",
    "#twice per\n",
    "\n",
    "data2perweek = pd.read_csv('C:/Users/KatherineRidley/OneDrive - Mint Diagnostics Ltd/Documents/Data Analysis/twiceaweek.csv')\n",
    "\n",
    "#every other\n",
    "\n",
    "dataeveryother = pd.read_csv('C:/Users/KatherineRidley/OneDrive - Mint Diagnostics Ltd/Documents/Data Analysis/everyother.csv')\n",
    "\n",
    "#all data\n",
    "\n",
    "data = pd.read_csv('C:/Users/KatherineRidley/OneDrive - Mint Diagnostics Ltd/Documents/Data Analysis/KR_MNC_CYCLES.csv')\n",
    "data = data.fillna(0)\n",
    "\n",
    "\n",
    "data['Ovulation - Baird DLT'] = data['Ovulation - Baird DLT'].replace(' ', 0)\n",
    "data['Ovulation - Baird DLT'] = data['Ovulation - Baird DLT'].replace('0', 0)\n",
    "data['Ovulation - Baird DLT'] = data['Ovulation - Baird DLT'].replace('1', 1)\n",
    "\n",
    "##segment data by date - weeky, 3 days etc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "accdict = {'Method':[], 'Frequency':[], 'n predicted positive':[], 'n predicted negative':[], 'LHp TP':[], 'LHp TN':[], 'LHp FP':[], 'LHp FN':[], 'LHp Accuracy':[], 'LHp Sensitivity':[], 'LHp Specificity':[], \n",
    "'CB TP':[], 'CB TN':[], 'CB FP':[], 'CB FN':[],\n",
    "'CB Accuracy':[], 'CB Sensitivity':[], 'CB Specificity':[], \n",
    "'LH-CB TP':[], 'LH-CB TN':[], 'LH-CB FP':[], 'LH-CB FN':[],\n",
    "'LH-CB Accuracy':[], 'LH-CB Sensitivity':[], 'LH-CB Specificity':[]}\n",
    "\n",
    "methods = ['Ovulation - Kassam', 'Ovulation - Baird DLT', 'E2 peak']\n",
    "\n",
    "\n",
    "\n",
    "datas = {'All data': data, 'Every 7th day': data1perweek, 'Every other day':dataeveryother, 'Twice a week': data2perweek}\n",
    "\n",
    "## list of indices where LH = 1\n",
    "\n",
    "LHindices = data.loc[data['Ovulation - LH peak'] == 1]\n",
    "\n",
    "## list of indices where CB = 1\n",
    "\n",
    "CBindices = data.loc[data['Ovulation - countback'] == 1]\n",
    "\n",
    "## list of indices where LH and CB = 1\n",
    "\n",
    "LHCBindices = data.loc[data['Ovulation - LH to countback']==1]\n",
    "\n",
    "for i in methods:\n",
    "\n",
    "    for k, d in datas.items():\n",
    "        print(str(i) + ' ' + str(k))\n",
    "    ## list of indices where method = 1\n",
    "        d[i] = d[i].fillna(0)\n",
    "\n",
    "        #print(data[i].head(50))\n",
    "        methodindices = d.loc[d[i] == 1]\n",
    "\n",
    "        method0indices = d.loc[d[i] == 0]\n",
    "        #print(data[i].value_counts())\n",
    "        #print(len(data[data[i]==1]))\n",
    "        #print(len(methodindices))\n",
    "        ## list of indices where method = 1 and LH = 1\n",
    "        LHTP = d[(d[i] == 1) & (d['Ovulation - LH peak'] == 1)]\n",
    "        #print(LHTP)\n",
    "        ## list of indices where method = 1 and CB = 1\n",
    "        CBTP = d[(d[i] == 1) & (d['Ovulation - countback'] == 1)]\n",
    "        ## list of indices where method = 1 and LH and CB = 1\n",
    "        LHCBTP = d[(d[i] == 1) & (d['Ovulation - LH to countback'] == 1)]\n",
    "        ## list of indices where method = 1 and LH = 0\n",
    "        LHFP = d[(d[i] == 1) & (d['Ovulation - LH peak'] == 0)]\n",
    "        ## list of indices where method = 1 and CB = 0\n",
    "        CBFP = d[(d[i] == 1) & (d['Ovulation - countback'] == 0)]\n",
    "        ## list of indices where method = 1 and LH and CB = 0\n",
    "        LHCBFP = d[(d[i] == 1) & (d['Ovulation - LH to countback'] == 0)]\n",
    "        ## list of indices where method = 0 and LH = 1\n",
    "        LHFN = d[(d[i] == 0) & (d['Ovulation - LH peak'] == 1)]\n",
    "        ## list of indices where method = 0 and CB = 1\n",
    "        CBFN = d[(d[i] == 0) & (d['Ovulation - countback'] == 1)]\n",
    "        ## list of indices where method = 0 and LH and CB = 1\n",
    "        LHCBFN= d[(d[i] == 0) & (d['Ovulation - LH to countback'] == 1)]\n",
    "        ## list of indices where method = 0 and LH = 0\n",
    "        LHTN = d[(d[i] == 0) & (d['Ovulation - LH peak'] == 0)]\n",
    "        #print(LHTN)\n",
    "        ## list of indices where method = 0 and CB = 0\n",
    "        CBTN = d[(d[i] == 0) & (d['Ovulation - countback'] == 0)]\n",
    "        ## list of indices where method = 0 and LH and CB = 0\n",
    "        LHCBTN = d[(d[i] == 0) & (d['Ovulation - LH to countback'] == 0)]\n",
    "        ## list of indices where method = 0\n",
    "        #method0indices = data[data[i] == 0].index.tolist()\n",
    "        ## list of indices where method = 0 and LH = 1\n",
    "        print(i)\n",
    "        ## generate metrics and tp and tn\n",
    "\n",
    "        LHaccuracy = (len(LHTP)+len(LHTN))/(len(LHTP)+len(LHFP)+len(LHFN)+len(LHTN))\n",
    "        if (len(LHTP)+len(LHFN)) == 0:\n",
    "            LHsensitivity = 0\n",
    "        else:\n",
    "            LHsensitivity = len(LHTP)/(len(LHTP)+len(LHFN))\n",
    "        if (len(LHTN)+len(LHFP)) == 0:\n",
    "            LHspecificity = 0\n",
    "        else:\n",
    "            LHspecificity = len(LHTN)/(len(LHTN)+len(LHFP))\n",
    "\n",
    "        CBaccuracy = (len(CBTP)+len(CBTN))/(len(CBTP)+len(CBFP)+len(CBFN)+len(CBTN))\n",
    "        if (len(CBTP)+len(CBFN)) == 0:\n",
    "            CBsensitivity = 0\n",
    "        else:\n",
    "\n",
    "            CBsensitivity = len(CBTP)/(len(CBTP)+len(CBFN))\n",
    "        if (len(CBTN)+len(CBFP)) == 0:\n",
    "            CBspecificity = 0\n",
    "        else:\n",
    "            CBspecificity = len(CBTN)/(len(CBTN)+len(CBFP))\n",
    "        LHCBaccuracy = (len(LHCBTP)+len(LHCBTN))/(len(LHCBTP)+len(LHCBFP)+len(LHCBFN)+len(LHCBTN))\n",
    "        if (len(LHCBTP)+len(LHCBFN)) == 0:\n",
    "            LHCBsensitivity = 0\n",
    "        else:\n",
    "\n",
    "            LHCBsensitivity = len(LHCBTP)/(len(LHCBTP)+len(LHCBFN))\n",
    "        if (len(LHCBTN)+len(LHCBFP)) == 0:\n",
    "            LHCBspecificity = 0\n",
    "        else:\n",
    "            LHCBspecificity = len(LHCBTN)/(len(LHCBTN)+len(LHCBFP))\n",
    "\n",
    "        '''print('LH accuracy: ', LHaccuracy)\n",
    "        print('CB accuracy: ', CBaccuracy)\n",
    "        print('LH and CB accuracy: ', LHCBaccuracy)\n",
    "        print('LH sensitivity: ', LHsensitivity)\n",
    "        print('CB sensitivity: ', CBsensitivity)\n",
    "        print('LH and CB sensitivity: ', LHCBsensitivity)\n",
    "        print('LH specificity: ', LHspecificity)\n",
    "        print('CB specificity: ', CBspecificity)\n",
    "        print('LH and CB specificity: ', LHCBspecificity)'''\n",
    "\n",
    "        accdict['Method'].append(i)\n",
    "        accdict['Frequency'].append(k)\n",
    "        accdict['LHp Accuracy'].append(LHaccuracy)\n",
    "        accdict['CB Accuracy'].append(CBaccuracy)\n",
    "        accdict['LH-CB Accuracy'].append(LHCBaccuracy)\n",
    "        accdict['LHp Sensitivity'].append(LHsensitivity)\n",
    "        accdict['CB Sensitivity'].append(CBsensitivity)\n",
    "        accdict['LH-CB Sensitivity'].append(LHCBsensitivity)\n",
    "        accdict['LHp Specificity'].append(LHspecificity)\n",
    "        accdict['CB Specificity'].append(CBspecificity)\n",
    "        accdict['LH-CB Specificity'].append(LHCBspecificity)\n",
    "        accdict['n predicted positive'].append(len(methodindices))\n",
    "        accdict['n predicted negative'].append(len(method0indices))\n",
    "        accdict['LHp TP'].append(len(LHTP))\n",
    "        accdict['CB TP'].append(len(CBTP))\n",
    "        accdict['LH-CB TP'].append(len(LHCBTP))\n",
    "        accdict['LHp FP'].append(len(LHFP))\n",
    "        accdict['CB FP'].append(len(CBFP))\n",
    "        accdict['LH-CB FP'].append(len(LHCBFP))\n",
    "        accdict['LHp FN'].append(len(LHFN))\n",
    "        accdict['CB FN'].append(len(CBFN))\n",
    "        accdict['LH-CB FN'].append(len(LHCBFN))\n",
    "        accdict['LHp TN'].append(len(LHTN))\n",
    "        accdict['CB TN'].append(len(CBTN))\n",
    "        accdict['LH-CB TN'].append(len(LHCBTN))\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "accdf  = pd.DataFrame(accdict)\n",
    "\n",
    "print(accdf)\n",
    "\n",
    "#to csv\n",
    "\n",
    "accdf.to_csv('C:/Users/KatherineRidley/OneDrive - Mint Diagnostics Ltd/Documents/Data Analysis/ovulation_day_accuracy.csv')\n",
    "\n",
    "## sensitivity specificity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cycledict = {'Player':[], 'Cycle':[], 'Frequency':[], 'LH peak day':[], 'Countback day':[], 'Kassam day':[], 'Kassam anov':[], 'Baird day':[], 'Baird anov':[], 'LH Kassam-3':[], 'LH Baird+-1':[], 'CB Kassam-3':[], 'CB Baird+-1':[]}\n",
    "\n",
    "for i in list(data.Player.unique()):\n",
    "    for k in [1,2,3]:\n",
    "        \n",
    "        datas = {'All data': data, 'Every 7th day': data1perweek, 'Every other day':dataeveryother, 'Twice a week': data2perweek}\n",
    "        for j, d in datas.items():\n",
    "            d = d[(d.Player == i) & (d['Cycle '] == k)].reset_index(drop=True)\n",
    "            cycledict['Player'].append(i)\n",
    "            cycledict['Cycle'].append(k)\n",
    "            cycledict['Frequency'].append(j)\n",
    "            \n",
    "            if len(d[d['Ovulation - LH peak']==1])==0:\n",
    "                cycledict['LH peak day'].append(0)\n",
    "                cycledict['Countback day'].append(0)\n",
    "                cycledict['Kassam day'].append(0)\n",
    "                cycledict['Kassam anov'].append(1)\n",
    "                cycledict['Baird day'].append(0)\n",
    "                cycledict['Baird anov'].append(1)\n",
    "                cycledict['LH Kassam-3'].append(0)\n",
    "                cycledict['LH Baird+-1'].append(0)\n",
    "                cycledict['CB Kassam-3'].append(0)\n",
    "                cycledict['CB Baird+-1'].append(0)\n",
    "                \n",
    "                continue\n",
    "            else:\n",
    "                cycledict['LH peak day'].append(d.loc[d['Ovulation - LH peak'] == 1, 'Day'].iloc[0])\n",
    "\n",
    "                cycledict['Countback day'].append(d.loc[d['Ovulation - countback']==1, 'Day'].iloc[0])\n",
    "\n",
    "                if len(d[d['Ovulation - Kassam']==1]) == 0:\n",
    "                    cycledict['Kassam day'].append(0)\n",
    "                    cycledict['Kassam anov'].append(1)\n",
    "                    kassamday = 0\n",
    "                    kassamd = d[0:0]\n",
    "\n",
    "                else:\n",
    "                    cycledict['Kassam day'].append(d.loc[d['Ovulation - Kassam']==1, 'Day'].iloc[0])\n",
    "                    cycledict['Kassam anov'].append(0)\n",
    "                    kassamday = d.loc[d['Ovulation - Kassam']==1, 'Ovulation - Kassam'].iloc[0]\n",
    "                    \n",
    "            \n",
    "                    #print(d['Ovulation - Kassam'])\n",
    "\n",
    "                    ind = list(d['Ovulation - Kassam']).index(kassamday)\n",
    "\n",
    "                    #basel = indata[column].iloc[ind:(ind+(base+1))].mean()\n",
    "               \n",
    "                    kassamd = d.iloc[ind - 5:ind+1]\n",
    "                    #print(kassamday-5, kassamday+1)\n",
    "\n",
    "                    \n",
    "\n",
    "                    \n",
    "                if len(d[d['Ovulation - Baird DLT']==1]) == 0:\n",
    "                    cycledict['Baird day'].append(0)\n",
    "                    cycledict['Baird anov'].append(1)\n",
    "                    bairdd = d[0:0]\n",
    "                else:\n",
    "                    cycledict['Baird day'].append(d[d['Ovulation - Baird DLT']==1]['Day'].iloc[0])\n",
    "                    cycledict['Baird anov'].append(0)\n",
    "                    bairdday = d[d['Ovulation - Baird DLT']==1]['Ovulation - Baird DLT'].iloc[0]\n",
    "                    ind = list(d['Ovulation - Baird DLT']).index(bairdday)\n",
    "\n",
    "                    bairdd = d.iloc[ind - 2:ind + 2]\n",
    "                \n",
    "\n",
    "                \n",
    "                if len(kassamd[kassamd['Ovulation - LH peak']==1]) == 0:\n",
    "                        cycledict['LH Kassam-3'].append(0)\n",
    "                else:\n",
    "                        cycledict['LH Kassam-3'].append(1)\n",
    "                \n",
    "\n",
    "                \n",
    "                if len(bairdd[bairdd['Ovulation - LH peak']==1]) == 0:\n",
    "                    cycledict['LH Baird+-1'].append(0)\n",
    "                else:\n",
    "                    cycledict['LH Baird+-1'].append(1)\n",
    "                \n",
    "\n",
    "                if len (kassamd[kassamd['Ovulation - countback']==1]) == 0:\n",
    "                    cycledict['CB Kassam-3'].append(0)\n",
    "                else:\n",
    "                    cycledict['CB Kassam-3'].append(1)\n",
    "\n",
    "                if len(bairdd[bairdd['Ovulation - countback']==1]) == 0:\n",
    "                    cycledict['CB Baird+-1'].append(0)\n",
    "                else:\n",
    "                    cycledict['CB Baird+-1'].append(1)\n",
    "\n",
    "                #print(i, k, j, d.loc[d['Ovulation - LH peak'] == 1, 'Day'].iloc[0], d.loc[d['Ovulation - countback']==1, 'Day'].iloc[0], kassamday, bairdday)\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "cycledf = pd.DataFrame(cycledict)\n",
    "\n",
    "print(cycledf)\n",
    "\n",
    "cycledf.to_csv('C:/Users/KatherineRidley/OneDrive - Mint Diagnostics Ltd/Documents/Data Analysis/ovulation_cycles.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##METRICS FOR CYCLE DF\n",
    "\n",
    "#sensitivity and specificity\n",
    "\n",
    "#sensitivity = true positive / (true positive + false negative)\n",
    "\n",
    "#specificity = true negative / (true negative + false positive)\n",
    "\n",
    "#true positive = ovulation day predicted by algorithm and confirmed by LH peak\n",
    "\n",
    "d = cycledf\n",
    "\n",
    "#sensitivity and specificity for LH peak\n",
    "\n",
    "newdict = {'Frequency':[], 'Kassam-3 LH accuracy':[], 'Baird+-1 LH accuracy':[], 'Kassam-3 CB accuracy':[], 'Baird+-1 CB accuracy':[]}\n",
    "\n",
    "\n",
    "        \n",
    "for j in d['Frequency'].unique():\n",
    "\n",
    "            \n",
    "            d = cycledf\n",
    "\n",
    "            d = d[(d['Frequency']==j)].reset_index(drop=True)\n",
    "\n",
    "            if len(d) == 0:\n",
    "                continue\n",
    "            else:\n",
    "                \n",
    "\n",
    "                newdict['Frequency'].append(j)\n",
    "                \n",
    "                tp = len(d[(d['Kassam anov']==0) & (d['LH Kassam-3']==1)])\n",
    "                fn = len(d[(d['Kassam anov']==0) & (d['LH Kassam-3']==0)])\n",
    "                #print(j, tp, fn)\n",
    "                tn = 0\n",
    "                fp = 0\n",
    "\n",
    "                newdict['Kassam-3 LH accuracy'].append((tp+tn)/(tp+tn+fp+fn))\n",
    "                #newdict['LH sensitivity'].append(tp/(tp+fn))\n",
    "                #newdict['LH specificity'].append(tn/(tn+fp))\n",
    "\n",
    "                tp = len(d[(d['Baird anov']==0) & (d['LH Baird+-1']==1)])\n",
    "                fn = len(d[(d['Baird anov']==0) &  (d['LH Baird+-1']==0)])\n",
    "                #print(i, k, j, tp, fn)\n",
    "                tn = 0\n",
    "                fp = 0\n",
    "\n",
    "                newdict['Baird+-1 LH accuracy'].append((tp+tn)/(tp+tn+fp+fn))\n",
    "                #newdict['CB sensitivity'].append(tp/(tp+fn))\n",
    "                #newdict['CB specificity'].append(tn/(tn+fp))\n",
    "\n",
    "                tp = len(d[(d['Kassam anov']==0) & (d['CB Kassam-3']==1)])\n",
    "                    \n",
    "                fn = len(d[(d['Kassam anov']==0) & (d['CB Kassam-3']==0)])\n",
    "                #print(i, k, j, tp, fn)\n",
    "                tn = 0\n",
    "                fp = 0\n",
    "\n",
    "                newdict['Kassam-3 CB accuracy'].append((tp+tn)/(tp+tn+fp+fn))\n",
    "                #newdict['LH sensitivity'].append(tp/(tp+fn))\n",
    "                #newdict['LH specificity'].append(tn/(tn+fp))\n",
    "\n",
    "                tp = len(d[(d['Baird anov']==0) & (d['CB Baird+-1']==1)])\n",
    "                    \n",
    "                fn = len(d[(d['Baird anov']==0) & (d['CB Baird+-1']==0)])\n",
    "                #print(i, k, j, tp, fn)\n",
    "                tn = 0\n",
    "                fp = 0\n",
    "\n",
    "                newdict['Baird+-1 CB accuracy'].append((tp+tn)/(tp+tn+fp+fn))\n",
    "                #newdict['CB sensitivity'].append(tp/(tp+fn))\n",
    "                #newdict['CB specificity'].append(tn/(tn+fp))\n",
    "                \n",
    "                \n",
    "\n",
    "newdf = pd.DataFrame(newdict)\n",
    "\n",
    "print(newdf)\n",
    "\n",
    "newdf.to_csv('C:/Users/KatherineRidley/OneDrive - Mint Diagnostics Ltd/Documents/Data Analysis/ovulation_metrics.csv')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##segment by date\n",
    "\n",
    "#print(data.Day)\n",
    "\n",
    "\n",
    "\n",
    "##for each player and cycle, find the first date of menses\n",
    "\n",
    "for i in data['Player'].unique():\n",
    "\n",
    "    for j in data['Cycle '].unique():\n",
    "\n",
    "        indata = data[(data['Player'] == i) & (data['Cycle '] == j)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if len(indata) >0:\n",
    "\n",
    "            \n",
    "\n",
    "            #print(data.loc[(data['Player'] == i) & (data['Cycle '] == j), 'Date'])\n",
    "\n",
    "            \n",
    "\n",
    "            data.loc[(data['Player'] == i) & (data['Cycle '] == j), 'Days since menses'] = data.loc[(data['Player'] == i) & (data['Cycle '] == j), 'Day'].astype(int)\n",
    "\n",
    "            ##make column == 1 for every 3rd Day\n",
    "\n",
    "            data.loc[(data['Player'] == i) & (data['Cycle '] == j) & ((data['Day']%3==0) | (data['Day']==1) | (data['Ovulation - LH peak']==1)), 'Every 3rd day'] = 1\n",
    "\n",
    "            ##every 7th day\n",
    "\n",
    "            data.loc[(data['Player'] == i) & (data['Cycle '] == j) & ((data['Day']%7==0) | (data['Day']==1) | (data['Ovulation - LH peak']==1)) , 'Every 7th day'] = 1\n",
    "\n",
    "            # every other day\n",
    "\n",
    "            data.loc[(data['Player'] == i) & (data['Cycle '] == j) & ((data['Day']%2==0) | (data['Ovulation - LH peak']==1)), 'Every other day'] = 1\n",
    "\n",
    "            ## twice a week\n",
    "\n",
    "            data.loc[(data['Player'] == i) & (data['Cycle '] == j)& ((data['Day']%4==0) | (data['Day']==1) | (data['Ovulation - LH peak']==1)), 'Twice a week'] = 1\n",
    "\n",
    "\n",
    "data = data.fillna(0)\n",
    "\n",
    "\n",
    "data1perweek = data[data['Every 7th day']==1]\n",
    "dataeveryother = data[data['Every other day']==1]\n",
    "dataevery3rd = data[data['Every 3rd day']==1]\n",
    "datetwiceaweek = data[data['Twice a week']==1]\n",
    "\n",
    "data1perweek.to_csv('C:/Users/KatherineRidley/OneDrive - Mint Diagnostics Ltd/Documents/Data Analysis/1perweek.csv')\n",
    "dataeveryother.to_csv('C:/Users/KatherineRidley/OneDrive - Mint Diagnostics Ltd/Documents/Data Analysis/everyother.csv')\n",
    "dataevery3rd.to_csv('C:/Users/KatherineRidley/OneDrive - Mint Diagnostics Ltd/Documents/Data Analysis/every3rd.csv')\n",
    "datetwiceaweek.to_csv('C:/Users/KatherineRidley/OneDrive - Mint Diagnostics Ltd/Documents/Data Analysis/twiceaweek.csv')\n",
    "\n",
    "\n",
    "\n",
    "            ##column weekly measurement\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1perweek = data[data['Every 7th day']==1]\n",
    "dataeveryother = data[data['Every other day']==1]\n",
    "dataevery3rd = data[data['Every 3rd day']==1]\n",
    "datetwiceaweek = data[data['Twice a week']==1]\n",
    "\n",
    "data1perweek.to_csv('C:/Users/KatherineRidley/OneDrive - Mint Diagnostics Ltd/Documents/Data Analysis/1perweek.csv')\n",
    "dataeveryother.to_csv('C:/Users/KatherineRidley/OneDrive - Mint Diagnostics Ltd/Documents/Data Analysis/everyother.csv')\n",
    "dataevery3rd.to_csv('C:/Users/KatherineRidley/OneDrive - Mint Diagnostics Ltd/Documents/Data Analysis/every3rd.csv')\n",
    "datetwiceaweek.to_csv('C:/Users/KatherineRidley/OneDrive - Mint Diagnostics Ltd/Documents/Data Analysis/twiceaweek.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##test accuracy metrics against LH peak\n",
    "\n",
    "\n",
    "data = data.fillna(0)\n",
    "\n",
    "##segment data by date - weeky, 3 days etc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "accdict = {'Method':[], 'Frequency':[], 'n predicted positive':[], 'n predicted negative':[], 'LHp TP':[], 'LHp TN':[], 'LHp FP':[], 'LHp FN':[], 'LHp Accuracy':[], 'LHp Sensitivity':[], 'LHp Specificity':[], \n",
    "'CB TP':[], 'CB TN':[], 'CB FP':[], 'CB FN':[],\n",
    "'CB Accuracy':[], 'CB Sensitivity':[], 'CB Specificity':[], \n",
    "'LH-CB TP':[], 'LH-CB TN':[], 'LH-CB FP':[], 'LH-CB FN':[],\n",
    "'LH-CB Accuracy':[], 'LH-CB Sensitivity':[], 'LH-CB Specificity':[]}\n",
    "\n",
    "methods = ['Ovulation - LH and Kassam 1', 'Ovulation - Baird DLT', 'Ovulation - Baird DLT region', 'E2 peak', 'E2 peak sustained']\n",
    "\n",
    "data1perweek = data[data['Every 7th day']==1]\n",
    "dataeveryother = data[data['Every other day']==1]\n",
    "dataevery3rd = data[data['Every 3rd day']==1]\n",
    "datetwiceaweek = data[data['Twice a week']==1]\n",
    "\n",
    "datas = {'All data': data, 'Every 7th day': data1perweek, 'Every other day':dataeveryother, 'Every 3rd day':dataevery3rd, 'Twice a week': datetwiceaweek}\n",
    "\n",
    "## list of indices where LH = 1\n",
    "\n",
    "LHindices = data.loc[data['Ovulation - LH peak'] == 1]\n",
    "\n",
    "## list of indices where CB = 1\n",
    "\n",
    "CBindices = data.loc[data['Ovulation - countback'] == 1]\n",
    "\n",
    "## list of indices where LH and CB = 1\n",
    "\n",
    "LHCBindices = data.loc[data['Ovulation - LH to countback']==1]\n",
    "\n",
    "for i in methods:\n",
    "\n",
    "    for k, d in datas.items():\n",
    "        print(str(i) + ' ' + str(k))\n",
    "    ## list of indices where method = 1\n",
    "        d[i] = d[i].fillna(0)\n",
    "\n",
    "        #print(data[i].head(50))\n",
    "        methodindices = d.loc[d[i] == 1]\n",
    "\n",
    "        method0indices = d.loc[d[i] == 0]\n",
    "        #print(data[i].value_counts())\n",
    "        #print(len(data[data[i]==1]))\n",
    "        #print(len(methodindices))\n",
    "        ## list of indices where method = 1 and LH = 1\n",
    "        LHTP = d[(d[i] == 1) & (d['Ovulation - LH peak'] == 1)]\n",
    "        #print(LHTP)\n",
    "        ## list of indices where method = 1 and CB = 1\n",
    "        CBTP = d[(d[i] == 1) & (d['Ovulation - countback'] == 1)]\n",
    "        ## list of indices where method = 1 and LH and CB = 1\n",
    "        LHCBTP = d[(d[i] == 1) & (d['Ovulation - LH to countback'] == 1)]\n",
    "        ## list of indices where method = 1 and LH = 0\n",
    "        LHFP = d[(d[i] == 1) & (d['Ovulation - LH peak'] == 0)]\n",
    "        ## list of indices where method = 1 and CB = 0\n",
    "        CBFP = d[(d[i] == 1) & (d['Ovulation - countback'] == 0)]\n",
    "        ## list of indices where method = 1 and LH and CB = 0\n",
    "        LHCBFP = d[(d[i] == 1) & (d['Ovulation - LH to countback'] == 0)]\n",
    "        ## list of indices where method = 0 and LH = 1\n",
    "        LHFN = d[(d[i] == 0) & (d['Ovulation - LH peak'] == 1)]\n",
    "        ## list of indices where method = 0 and CB = 1\n",
    "        CBFN = d[(d[i] == 0) & (d['Ovulation - countback'] == 1)]\n",
    "        ## list of indices where method = 0 and LH and CB = 1\n",
    "        LHCBFN= d[(d[i] == 0) & (d['Ovulation - LH to countback'] == 1)]\n",
    "        ## list of indices where method = 0 and LH = 0\n",
    "        LHTN = d[(d[i] == 0) & (d['Ovulation - LH peak'] == 0)]\n",
    "        #print(LHTN)\n",
    "        ## list of indices where method = 0 and CB = 0\n",
    "        CBTN = d[(d[i] == 0) & (d['Ovulation - countback'] == 0)]\n",
    "        ## list of indices where method = 0 and LH and CB = 0\n",
    "        LHCBTN = d[(d[i] == 0) & (d['Ovulation - LH to countback'] == 0)]\n",
    "        ## list of indices where method = 0\n",
    "        #method0indices = data[data[i] == 0].index.tolist()\n",
    "        ## list of indices where method = 0 and LH = 1\n",
    "        print(i)\n",
    "        ## generate metrics and tp and tn\n",
    "\n",
    "        LHaccuracy = (len(LHTP)+len(LHTN))/(len(LHTP)+len(LHFP)+len(LHFN)+len(LHTN))\n",
    "        if (len(LHTP)+len(LHFN)) == 0:\n",
    "            LHsensitivity = 0\n",
    "        else:\n",
    "            LHsensitivity = len(LHTP)/(len(LHTP)+len(LHFN))\n",
    "        if (len(LHTN)+len(LHFP)) == 0:\n",
    "            LHspecificity = 0\n",
    "        else:\n",
    "            LHspecificity = len(LHTN)/(len(LHTN)+len(LHFP))\n",
    "\n",
    "        CBaccuracy = (len(CBTP)+len(CBTN))/(len(CBTP)+len(CBFP)+len(CBFN)+len(CBTN))\n",
    "        if (len(CBTP)+len(CBFN)) == 0:\n",
    "            CBsensitivity = 0\n",
    "        else:\n",
    "\n",
    "            CBsensitivity = len(CBTP)/(len(CBTP)+len(CBFN))\n",
    "        if (len(CBTN)+len(CBFP)) == 0:\n",
    "            CBspecificity = 0\n",
    "        else:\n",
    "            CBspecificity = len(CBTN)/(len(CBTN)+len(CBFP))\n",
    "        LHCBaccuracy = (len(LHCBTP)+len(LHCBTN))/(len(LHCBTP)+len(LHCBFP)+len(LHCBFN)+len(LHCBTN))\n",
    "        if (len(LHCBTP)+len(LHCBFN)) == 0:\n",
    "            LHCBsensitivity = 0\n",
    "        else:\n",
    "\n",
    "            LHCBsensitivity = len(LHCBTP)/(len(LHCBTP)+len(LHCBFN))\n",
    "        if (len(LHCBTN)+len(LHCBFP)) == 0:\n",
    "            LHCBspecificity = 0\n",
    "        else:\n",
    "            LHCBspecificity = len(LHCBTN)/(len(LHCBTN)+len(LHCBFP))\n",
    "\n",
    "        '''print('LH accuracy: ', LHaccuracy)\n",
    "        print('CB accuracy: ', CBaccuracy)\n",
    "        print('LH and CB accuracy: ', LHCBaccuracy)\n",
    "        print('LH sensitivity: ', LHsensitivity)\n",
    "        print('CB sensitivity: ', CBsensitivity)\n",
    "        print('LH and CB sensitivity: ', LHCBsensitivity)\n",
    "        print('LH specificity: ', LHspecificity)\n",
    "        print('CB specificity: ', CBspecificity)\n",
    "        print('LH and CB specificity: ', LHCBspecificity)'''\n",
    "\n",
    "        accdict['Method'].append(i)\n",
    "        accdict['Frequency'].append(k)\n",
    "        accdict['LHp Accuracy'].append(LHaccuracy)\n",
    "        accdict['CB Accuracy'].append(CBaccuracy)\n",
    "        accdict['LH-CB Accuracy'].append(LHCBaccuracy)\n",
    "        accdict['LHp Sensitivity'].append(LHsensitivity)\n",
    "        accdict['CB Sensitivity'].append(CBsensitivity)\n",
    "        accdict['LH-CB Sensitivity'].append(LHCBsensitivity)\n",
    "        accdict['LHp Specificity'].append(LHspecificity)\n",
    "        accdict['CB Specificity'].append(CBspecificity)\n",
    "        accdict['LH-CB Specificity'].append(LHCBspecificity)\n",
    "        accdict['n predicted positive'].append(len(methodindices))\n",
    "        accdict['n predicted negative'].append(len(method0indices))\n",
    "        accdict['LHp TP'].append(len(LHTP))\n",
    "        accdict['CB TP'].append(len(CBTP))\n",
    "        accdict['LH-CB TP'].append(len(LHCBTP))\n",
    "        accdict['LHp FP'].append(len(LHFP))\n",
    "        accdict['CB FP'].append(len(CBFP))\n",
    "        accdict['LH-CB FP'].append(len(LHCBFP))\n",
    "        accdict['LHp FN'].append(len(LHFN))\n",
    "        accdict['CB FN'].append(len(CBFN))\n",
    "        accdict['LH-CB FN'].append(len(LHCBFN))\n",
    "        accdict['LHp TN'].append(len(LHTN))\n",
    "        accdict['CB TN'].append(len(CBTN))\n",
    "        accdict['LH-CB TN'].append(len(LHCBTN))\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "accdf  = pd.DataFrame(accdict)\n",
    "\n",
    "print(accdf)\n",
    "\n",
    "accdf.to_csv(\"C:/Users/KatherineRidley/Mint Diagnostics Ltd/Hormone Data Analysis - General/Notebooks/MNC/MNCOVAnalysis_v1.csv\")\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    # if data[i] == 1 & data['Ovulation - LH peak'] == 1:\n",
    "    #     print('true positive')\n",
    "    # elif data[i] == 0 & data['Ovulation - LH peak'] == 0:\n",
    "    #     print('true negative')\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ratio e1 to p4\n",
    "\n",
    "data['E2/P4'] = (data['E2'] / data['P4'])*100\n",
    "\n",
    "print(data['E2_difference'].describe())\n",
    "\n",
    "## print data with e2_difference > 100\n",
    "\n",
    "#print(data[data['E2_difference'] > 200])\n",
    "\n",
    "#print(data[data['E2_difference'] == data['E2_difference'].max()])\n",
    "\n",
    "print(data.head(100))\n",
    "\n",
    "data.to_csv(\"C:/Users/KatherineRidley/OneDrive - Mint Diagnostics Ltd/Documents/Data Analysis/KR_MNC_CYCLES.csv\", index=False)\n",
    "\n",
    "data.rename(columns={'Ovulation - LH and Kassam 1': 'Ovulation - Kassam 1'}, inplace=True)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculate accuracy of LH peak to kassam method\n",
    "\n",
    "data.rename(columns={'Ovulation - LH and Kassam 1': 'Ovulation - Kassam 1'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split the data into training and testing sets\n",
    "'''for i in data['Player'].unique():\n",
    "    for j in data['Cycle '].unique():\n",
    "        \n",
    "        data2 = data[(data['Player'] == i) & (data['Cycle '] == j)]\n",
    "        print('Player:', i, 'Cycle:', j)\n",
    "        \n",
    "\n",
    "        if len(data2) >0:\n",
    "            #print(data2.head(20)'''\n",
    "        \n",
    "df = data.drop(['Ovulation - LH peak', 'Date'], axis=1).fillna(0)\n",
    "print(len(df))\n",
    "labels = data['Ovulation - LH peak'].fillna(0)\n",
    "\n",
    "print('labels:', set(labels))\n",
    "print(len(labels))\n",
    "#print(labels)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, labels, test_size=0.5, random_state=0)\n",
    "\n",
    "# Train the logistic regression model on the training data\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the ovulation day on the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Predicted ovulation day:\", y_pred)\n",
    "\n",
    "# Print the model accuracy, which is the average of the correct predictions\n",
    "\n",
    "print(\"Model accuracy:\", clf.score(X_test, y_test))\n",
    "\n",
    "##metrics\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(confusion_matrix)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "logit_roc_auc = roc_auc_score(y_test, clf.predict(X_test))\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, clf.predict_proba(X_test)[:,1])\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n",
    "\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "\n",
    "plt.ylim([0.0, 1.05])\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "\n",
    "plt.ylabel('True Positive Rate')\n",
    "\n",
    "plt.title('Receiver operating characteristic')\n",
    "\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.savefig('Log_ROC')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Plot the data\n",
    "\n",
    "'''plt.plot(data['Date'], data['E2'], label='Estradiol')\n",
    "\n",
    "plt.plot(data['Date'], data['P4'], label='Progesterone')\n",
    "\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "plt.xlabel('Date')\n",
    "\n",
    "plt.ylabel('Hormone levels')\n",
    "\n",
    "plt.title('Estradiol and Progesterone levels across menstrual cycle')\n",
    "\n",
    "# Find the day of ovulation by finding the day with the highest estradiol level\n",
    "\n",
    "ovulation_day = data['Date'][data['E2_difference'].idxmax()]\n",
    "\n",
    "print(\"Ovulation day by E2 level :\", ovulation_day)\n",
    "\n",
    "print(\"Ovulation day:\", data['Date'][data['Ovulation - LH peak'].idxmax()])'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ov_dict = {'Player': [], 'Cycle': [], 'Ovulation day - LH': [], 'Cycle length':[], 'Ovulation day - countback': [], 'Day - P4 peak': [], 'P4 baseline':[],  'Day - E2 peak': [], 'E2 baseline': []}\n",
    "\n",
    "for i in data['Player'].unique():\n",
    "    for j in data['Cycle '].unique():\n",
    "        indata = data[(data['Player'] == i) & (data['Cycle '] == j)]\n",
    "        if len(indata) >0:\n",
    "            ov_dict['Player'].append(i)\n",
    "            ov_dict['Cycle'].append(j)\n",
    "            ov_dict['Ovulation day - LH'].append(indata['Day'][indata['Ovulation - LH peak'].idxmax()])\n",
    "            ov_dict['Cycle length'].append(len(indata))\n",
    "            ov_dict['Ovulation day - countback'].append(indata['Day'][indata['Ovulation - countback'].idxmax()])\n",
    "            ov_dict['Day - P4 peak'].append(indata['Day'][indata['P4_difference'].idxmax()])\n",
    "            e2base = indata['E2'].iloc[0:6].mean()\n",
    "            P4base = indata['P4'].iloc[0:6].mean()\n",
    "            ov_dict['P4 baseline'].append(P4base)\n",
    "            ov_dict['E2 baseline'].append(e2base)\n",
    "            ov_dict['Day - E2 peak'].append(indata['Day'][indata['E2_difference'].idxmax()])\n",
    "\n",
    "ov_df = pd.DataFrame(ov_dict)\n",
    "ov_df['Ovulation day - LH'] = ov_df['Ovulation day - LH'].astype(int)\n",
    "ov_df['Cycle length'] = ov_df['Cycle length'].astype(int)\n",
    "ov_df['Ovulation day - countback'] = ov_df['Ovulation day - countback'].astype(int)\n",
    "ov_df['Day - P4 peak'] = ov_df['Day - P4 peak'].astype(int)\n",
    "ov_df['Day - E2 peak'] = ov_df['Day - E2 peak'].astype(int)\n",
    "ov_df['P4 baseline'] = ov_df['P4 baseline'].astype(int)\n",
    "ov_df['E2 baseline'] = ov_df['E2 baseline'].astype(int)\n",
    "ov_df['Ovulation day - LH - diff'] = ov_df['Ovulation day - LH'] - ov_df['Ovulation day - countback']\n",
    "\n",
    "print(ov_df.columns)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ov_df.drop(['Ovulation day - LH', 'Ovulation day - LH - diff'], axis=1).fillna(0)\n",
    "#print(len(df))\n",
    "labels = ov_df['Ovulation day - LH'].fillna(0)\n",
    "\n",
    "print('labels:', set(labels))\n",
    "#print(len(labels))\n",
    "#print(labels)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, labels, test_size=0.5, random_state=0)\n",
    "\n",
    "# Train the logistic regression model on the training data\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the ovulation day on the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Predicted ovulation day:\", y_pred)\n",
    "\n",
    "# Print the model accuracy, which is the average of the correct predictions\n",
    "\n",
    "print(\"Model accuracy:\", clf.score(X_test, y_test))\n",
    "\n",
    "##metrics\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(confusion_matrix)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "logit_roc_auc = roc_auc_score(y_test, clf.predict(X_test))\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, clf.predict_proba(X_test)[:,1])\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n",
    "\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "\n",
    "plt.ylim([0.0, 1.05])\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "\n",
    "plt.ylabel('True Positive Rate')\n",
    "\n",
    "plt.title('Receiver operating characteristic')\n",
    "\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.savefig('Log_ROC')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fe6768c439df95c45b1ef781494ca15f387e2cda4440aa40b48ca20f1d315678"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
